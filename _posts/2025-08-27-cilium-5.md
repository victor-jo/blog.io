---
layout: post
title: "Cilium 네트워킹 핸즈온 가이드 (5): Performance & Tuning"
date: 2025-08-27 10:00:00 +0900
categories: cilium kubernetes
tags: [cilium, performance, tuning, ebpf, scalability, kube-burner, clusterloader2, kubernetes]
---

## Cilium 네트워킹 핸즈온 가이드 (5)

Cilium과 Kubernetes의 성능 최적화는 대규모 클러스터 운영에 필수적입니다.  
이번 포스트에서는 K8S와 Cilium의 성능 측정, 분석, 튜닝 방법을 실습을 통해 알아보겠습니다.

### 실습 환경 소개

K8S 및 Cilium 성능 테스트를 위한 환경을 구성합니다.

- Control Plane(k8s-ctr): 192.168.10.100
- Worker Node 1(k8s-w1): 192.168.10.101  
- Router: 192.168.10.200 (라우팅 및 외부 접속 테스트용)

### 기본 환경 확인
```bash
# 클러스터 정보 확인
kubectl cluster-info
kubectl get node -owide

# Cilium 설치 상태 확인
cilium status
cilium config view | grep -E '^loadbalancer|l7|bpf'
```

---

## Kubernetes Performance

### K8S 성능 측정 환경 구성

Kind 클러스터와 모니터링 스택을 구성하여 성능 측정 환경을 준비합니다.

```bash
# Prometheus Target 설정이 포함된 Kind 클러스터 생성
kind create cluster --name myk8s --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  - containerPort: 30003
    hostPort: 30003
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    controllerManager:
      extraArgs:
        bind-address: 0.0.0.0
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: http://0.0.0.0:2381
    scheduler:
      extraArgs:
        bind-address: 0.0.0.0
  - |
    kind: KubeProxyConfiguration
    metricsBindAddress: 0.0.0.0
EOF

# kube-ops-view 설치
helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 \
  --set service.main.type=NodePort,service.main.ports.http.nodePort=30000 \
  --set env.TZ="Asia/Seoul" --namespace kube-system

# metrics-server 설치
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server \
  --set 'args[0]=--kubelet-insecure-tls' -n kube-system
```

### Kube-burner를 활용한 부하 테스트

Kube-burner는 Kubernetes 클러스터의 성능과 확장성을 테스트하는 도구입니다.

#### Kube-burner 설치
```bash
# Mac M1/M2
curl -LO https://github.com/kube-burner/kube-burner/releases/download/v1.17.3/kube-burner-V1.17.3-darwin-arm64.tar.gz
tar -xvf kube-burner-V1.17.3-darwin-arm64.tar.gz

sudo cp kube-burner /usr/local/bin
kube-burner version

rm -rf kube-burner-V1.17.3-darwin-arm64.tar.gz 
```

#### 쿠버네티스 모니터링 스택 설치
```bash
cat <<EOT > monitor-values.yaml
prometheus:
  prometheusSpec:
    scrapeInterval: "15s"
    evaluationInterval: "15s"
  service:
    type: NodePort
    nodePort: 30001

grafana:
  defaultDashboardsTimezone: Asia/Seoul
  adminPassword: prom-operator
  service:
    type: NodePort
    nodePort: 30002

alertmanager:
  enabled: false
defaultRules:
  create: false
prometheus-windows-exporter:
  prometheus:
    monitor:
      enabled: false
EOT

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 75.15.1 \
-f monitor-values.yaml --create-namespace --namespace monitoring
```

#### Grafana 대시보드 구성
```bash
# Grafana 마켓플레이스 https://grafana.com/grafana/dashboards/15661-k8s-dashboard-en-20250125/
wget https://grafana.com/api/dashboards/15661/revisions/2/download

# macOS grafana 웹 접속 (admin / prom-operator)
open http://127.0.0.1:30002
```

#### 디플로이먼트 생성 및 삭제 시나리오
```bash
# 설정 파일 생성
cat << EOF > s1-config.yaml
global:
  measurements:
    - name: none

jobs:
  - name: create-deployments
    jobType: create
    jobIterations: 10
    qps: 10
    burst: 10
    namespace: kube-burner-test
    namespaceLabels: {kube-burner-job: delete-me}
    waitWhenFinished: true
    verifyObjects: false
    preLoadImages: false
    objects:
      - objectTemplate: s1-deployment.yaml
        replicas: 2
EOF

# 디플로이먼트 템플릿
cat << EOF > s1-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-{{ .Iteration}}-{{.Replica}}
  labels:
    app: test-{{ .Iteration }}-{{.Replica}}
    kube-burner-job: delete-me
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-{{ .Iteration}}-{{.Replica}}
  template:
    metadata:
      labels:
        app: test-{{ .Iteration}}-{{.Replica}}
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          ports:
            - containerPort: 80
EOF

# 부하 발생 실행
kube-burner init -c s1-config.yaml --log-level debug

# 생성된 리소스 확인
kubectl get deploy -A -l kube-burner-job=delete-me
kubectl get pod -A -l kube-burner-job=delete-me
```

#### 대규모 부하 테스트 시나리오

Pod 수를 증가시키며 클러스터 한계를 테스트합니다.

```bash
# 150 Pod 생성 시나리오
cat << EOF > s2-config.yaml
global:
  measurements:
    - name: none
jobs:
  - name: stress-test-150
    jobType: create
    jobIterations: 30
    qps: 50
    burst: 100
    namespace: kube-burner-stress
    namespaceLabels: {kube-burner-job: stress-test}
    waitWhenFinished: true
    verifyObjects: true
    preLoadImages: false
    objects:
      - objectTemplate: s2-pod.yaml
        replicas: 5
EOF

cat << EOF > s2-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: stress-pod-{{ .Iteration}}-{{.Replica}}
  labels:
    kube-burner-job: stress-test
    iteration: "{{ .Iteration }}"
spec:
  containers:
  - name: stress
    image: busybox
    command: ["sleep", "3600"]
    resources:
      requests:
        cpu: "10m"
        memory: "10Mi"
      limits:
        cpu: "20m"
        memory: "20Mi"
EOF

# 150 Pod 생성 실행
kube-burner init -c s2-config.yaml --log-level debug

# Ruuning Pod 개수 확인
kubectl get pod -A -l kube-burner-job=stress-test | grep Running | wc -l

# Pending Pod 개수 확인
kubectl get pod -A -l kube-burner-job=stress-test | grep Pending | wc -l

# Pod가 Pending 상태인 이유 확인
kubectl describe pod -A -l kube-burner-job=stress-test | grep -A5 Events

# --
# Events:
#   Type     Reason            Age                 From               Message
#   ----     ------            ----                ----               -------
#   Warning  FailedScheduling  7m15s               default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
#   Warning  FailedScheduling  108s (x7 over 83m)  default-scheduler  0/1 nodes are available: 1 Too many pods. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.

# 노드의 maxPods 제한 확인 (기본값: 110개)
kubectl get nodes -o json | jq '.items[].status.capacity.pods'
kubectl get nodes -o json | jq '.items[].status.allocatable.pods'

# 현재 실행 중인 전체 Pod 수 확인
kubectl get pods --all-namespaces --no-headers | wc -l

# maxPods 설정 확인 및 수정
echo "=== Current maxPods Configuration ==="
kubectl get cm -n kube-system kubelet-config -o yaml | grep maxPods || echo "maxPods: 110 (default)"

# maxPods 제한 해결: 노드 내부에서 kubelet 설정 수정
docker exec -it myk8s-control-plane bash

# kubelet 설정 확인 (없으면 기본값 110개)
cat /var/lib/kubelet/config.yaml | grep maxPods

# vim 설치 및 설정 수정
apt update && apt install vim -y
vim /var/lib/kubelet/config.yaml
# maxPods: 400 추가 또는 수정

# kubelet 재시작
systemctl restart kubelet
systemctl status kubelet
exit

# 변경 확인
kubectl get nodes -o json | jq '.items[].status.allocatable.pods'

# 리소스 사용량 확인
kubectl top nodes

# NAME                  CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
# myk8s-control-plane   1006m        8%       3263Mi          20%     

# 300 Pod 생성 시나리오
cat << EOF > s3-config.yaml
global:
  measurements:
    - name: none
jobs:
  - name: stress-test-300
    jobType: create
    jobIterations: 60
    qps: 100
    burst: 200
    namespace: kube-burner-extreme
    namespaceLabels: {kube-burner-job: extreme-test}
    waitWhenFinished: true
    verifyObjects: true
    objects:
      - objectTemplate: s3-pod.yaml
        replicas: 5
EOF

cat << EOF > s3-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: stress-pod-{{ .Iteration}}-{{.Replica}}
  labels:
    kube-burner-job: extreme-test
    iteration: "{{ .Iteration }}"
spec:
  containers:
  - name: stress
    image: busybox
    command: ["sleep", "3600"]
    resources:
      requests:
        cpu: "10m"
        memory: "10Mi"
      limits:
        cpu: "20m"
        memory: "20Mi"
EOF

# 300 Pod 생성 실행
time kube-burner init -c s3-config.yaml

# 스케줄링 성능 확인
kubectl get pods -A -l kube-burner-job=extreme-test --no-headers | wc -l
kubectl get pods -A -l kube-burner-job=extreme-test --field-selector=status.phase!=Running --no-headers | wc -l
```

### API Server 성능 메트릭 분석

Prometheus를 통해 API Server의 성능 메트릭을 수집하고 분석합니다.

```bash
# MacOS 에서 프로메테우스 오픈
open http://localhost:30001

# API Server QPS 확인
sum by(resource, code, verb) (rate(apiserver_request_total{resource=~".+"}[5m]))

# API 요청 지연 시간 (P90)
histogram_quantile(0.90, sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (verb, le))

# etcd 요청 지연 시간
histogram_quantile(0.99, sum by(le, operation, type) (rate(etcd_request_duration_seconds_bucket[1m])))

# Work Queue 깊이
sum(rate(workqueue_depth{job="kube-controller-manager"}[5m])) by (instance, name)
```
---

## Cilium Performance

### Cilium 성능 측정 환경 구성

Cilium CNI가 설치된 클러스터에서 성능 테스트 환경을 구성합니다.

```bash
# Cilium CNI 설치 (성능 최적화 옵션 포함)
cilium install --version 1.18.1 \
  --set ipam.mode=kubernetes \
  --set ipv4NativeRoutingCIDR=172.20.0.0/16 \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  --set endpointRoutes.enabled=true \
  --set kubeProxyReplacement=true \
  --set bpf.masquerade=true \
  --set hubble.enabled=true \
  --set hubble.relay.enabled=true \
  --set hubble.ui.enabled=true \
  --set hubble.ui.service.type=NodePort \
  --set hubble.ui.service.nodePort=30003 \
  --set prometheus.enabled=true \
  --set operator.prometheus.enabled=true
```

### iperf3를 활용한 네트워크 성능 측정

Pod 간 네트워크 대역폭과 지연시간을 측정합니다.

```bash
# iperf3 서버 및 클라이언트 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iperf3-server
spec:
  selector:
    matchLabels:
      app: iperf3-server
  replicas: 1
  template:
    metadata:
      labels:
        app: iperf3-server
    spec:
      containers:
      - name: iperf3-server
        image: networkstatic/iperf3
        args: ["-s"]
        ports:
        - containerPort: 5201
---
apiVersion: v1
kind: Service
metadata:
  name: iperf3-server
spec:
  selector:
    app: iperf3-server
  ports:
    - protocol: TCP
      port: 5201
      targetPort: 5201
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iperf3-client
spec:
  selector:
    matchLabels:
      app: iperf3-client
  replicas: 1
  template:
    metadata:
      labels:
        app: iperf3-client
    spec:
      containers:
      - name: iperf3-client
        image: networkstatic/iperf3
        command: ["sleep"]
        args: ["infinity"]
EOF

# TCP 대역폭 테스트
kubectl exec -it deploy/iperf3-client -- iperf3 -c iperf3-server -t 10

# [ ID] Interval           Transfer     Bitrate         Retr  Cwnd
# [  5]   0.00-1.00   sec  12.5 GBytes   107 Gbits/sec  114   1.49 MBytes       
# [  5]   1.00-2.00   sec  12.3 GBytes   106 Gbits/sec    0   1.49 MBytes       
# [  5]   2.00-3.00   sec  12.3 GBytes   106 Gbits/sec   34   1.49 MBytes       
# [  5]   3.00-4.00   sec  12.8 GBytes   110 Gbits/sec  129   1.51 MBytes       
# [  5]   4.00-5.00   sec  13.1 GBytes   113 Gbits/sec    0   1.51 MBytes       
# [  5]   5.00-6.00   sec  13.4 GBytes   115 Gbits/sec    0   1.51 MBytes       
# [  5]   6.00-7.00   sec  13.6 GBytes   117 Gbits/sec    0   1.51 MBytes       
# [  5]   7.00-8.00   sec  13.5 GBytes   116 Gbits/sec   54   1.51 MBytes       
# [  5]   8.00-9.00   sec  13.7 GBytes   118 Gbits/sec    0   1.51 MBytes       
# [  5]   9.00-10.00  sec  13.4 GBytes   115 Gbits/sec    0   1.51 MBytes       
# - - - - - - - - - - - - - - - - - - - - - - - - -
# [ ID] Interval           Transfer     Bitrate         Retr
# [  5]   0.00-10.00  sec   131 GBytes   112 Gbits/sec  331             sender
# [  5]   0.00-10.00  sec   131 GBytes   112 Gbits/sec                  receiver 

# 양방향 테스트
kubectl exec -it deploy/iperf3-client -- iperf3 -c iperf3-server -t 5 --bidir

# [ ID][Role] Interval           Transfer     Bitrate         Retr  Cwnd
# [  5][TX-C]   0.00-1.00   sec  11.6 GBytes  99.6 Gbits/sec   28    964 KBytes       
# [  7][RX-C]   0.00-1.00   sec  7.18 GBytes  61.7 Gbits/sec                  
# [  5][TX-C]   1.00-2.00   sec  9.33 GBytes  80.1 Gbits/sec    0    964 KBytes       
# [  7][RX-C]   1.00-2.00   sec  7.79 GBytes  66.9 Gbits/sec                  
# [  5][TX-C]   2.00-3.00   sec  9.13 GBytes  78.4 Gbits/sec   44    964 KBytes       
# [  7][RX-C]   2.00-3.00   sec  7.54 GBytes  64.8 Gbits/sec                  
# [  5][TX-C]   3.00-4.00   sec  8.19 GBytes  70.4 Gbits/sec  105    964 KBytes       
# [  7][RX-C]   3.00-4.00   sec  7.12 GBytes  61.1 Gbits/sec                  
# [  5][TX-C]   4.00-5.00   sec  8.93 GBytes  76.7 Gbits/sec    0    964 KBytes       
# [  7][RX-C]   4.00-5.00   sec  8.10 GBytes  69.6 Gbits/sec                  
# - - - - - - - - - - - - - - - - - - - - - - - - -
# [ ID][Role] Interval           Transfer     Bitrate         Retr
# [  5][TX-C]   0.00-5.00   sec  47.2 GBytes  81.1 Gbits/sec  177             sender
# [  5][TX-C]   0.00-5.00   sec  47.2 GBytes  81.1 Gbits/sec                  receiver
# [  7][RX-C]   0.00-5.00   sec  37.7 GBytes  64.8 Gbits/sec  191             sender
# [  7][RX-C]   0.00-5.00   sec  37.7 GBytes  64.8 Gbits/sec                  receiver
```

### Cilium 연결성 테스트

Cilium의 내장 연결성 테스트 도구를 활용합니다.

```bash
# 기능 검증 테스트 (122개 시나리오)
cilium connectivity test --debug

# 성능 측정 테스트
cilium connectivity perf \
  --duration 30s \
  --pod-net \
  --host-net \
  --same-node \
  --other-node
```

---

## Cilium Tuning

### eBPF Map 최적화 심화 실습

대규모 클러스터를 위한 eBPF Map 크기 조정과 성능 최적화를 수행합니다.

#### eBPF Map 현황 분석

```bash
# 현재 eBPF Map 상태 확인
echo "=== Current eBPF Maps Status ==="
kubectl exec -n kube-system ds/cilium -- cilium bpf metrics list

# Map 별 사용량 확인
kubectl exec -n kube-system ds/cilium -- sh -c "
  echo '=== eBPF Map Usage ==='
  cilium map list | while read map; do
    if [ ! -z \"\$map\" ]; then
      echo \"Map: \$map\"
      cilium map get \$map 2>/dev/null | wc -l
    fi
  done
"

# Map: Name                           Num entries   Num errors   Cache enabled
# 0
# Map: cilium_lb4_services_v2         65            0            true
# ...
# Map: cilium_lb4_reverse_nat         31            0            true
# 32

# 시스템 메모리 확인
kubectl exec -n kube-system ds/cilium -- sh -c "
  echo '=== System Memory Info ==='
  free -h
  echo ''
  echo '=== eBPF Memory Usage ==='
  cat /proc/meminfo | grep -E 'BPF|Percpu'
"

# === System Memory Info ===
#                total        used        free      shared  buff/cache   available
# Mem:            15Gi        10Gi       3.4Gi       119Mi       2.0Gi       5.0Gi
# Swap:           16Gi       4.3Gi        12Gi

# === eBPF Memory Usage ===
# Percpu:            27168 kB

# Map 압력 지표 확인
kubectl exec -n kube-system ds/cilium -- cilium metrics list | grep -E "bpf_map_pressure|bpf_map_ops"

# cilium_bpf_map_ops_total                                                 map_name=ipcache_v2 operation=delete outcome=success                                                              2.000000
# cilium_bpf_map_ops_total                                                 map_name=ipcache_v2 operation=update outcome=success                                                              22.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_backends_v3 operation=delete outcome=success                                                         6.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_backends_v3 operation=update outcome=success                                                         61.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_reverse_nat operation=update outcome=success                                                         62.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_services_v2 operation=delete outcome=success                                                         6.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_services_v2 operation=update outcome=success                                                         145.000000
# cilium_bpf_map_ops_total                                                 map_name=lb_affinity_match operation=delete outcome=fail                                                          89.000000
# cilium_bpf_map_ops_total                                                 map_name=lxc operation=update outcome=success                                                                     17.000000
# cilium_bpf_map_ops_total                                                 map_name=policy operation=update outcome=success                                                                  20.000000
# cilium_bpf_map_ops_total                                                 map_name=runtime_config operation=update outcome=success                                                          297.000000
# cilium_bpf_map_pressure                                                  map_name=ct4_global                                                                                               0.002618
# cilium_bpf_map_pressure                                                  map_name=ct_any4_global                                                                                           0.006754
# cilium_bpf_map_pressure                                                  map_name=ipcache_v2                                                                                               0.000039
# cilium_bpf_map_pressure                                                  map_name=lxc                                                                                                      0.000122
```

#### eBPF Map 크기 동적 조정

```bash
# 메모리 기반 동적 크기 조정
cat << EOF > /tmp/bpf-map-tuning.yaml
bpf:
  # Map 크기를 시스템 메모리의 비율로 설정
  mapDynamicSizeRatio: 0.0025  # 0.25% (기본값: 0.0025)
  
  # 개별 Map 크기 설정
  policyMapMax: 16384      # 정책 Map 최대 크기
  lbMapMax: 65536         # 로드밸런서 Map 최대 크기
  natMapMax: 524288       # NAT Map 최대 크기
  neighMapMax: 524288     # Neighbor Map 최대 크기
  sockMapMax: 65535       # Socket Map 최대 크기
  
  # CT (Connection Tracking) Map 설정
  ctMapEntriesGlobalTCP: 524288
  ctMapEntriesGlobalAny: 262144
  
  # 정책 Map 항목 수
  policyMapEntries: 16384
  
  # Map 압력 관리
  mapPressure:
    enabled: true
    # Map이 가득 찰 때의 동작
    evictionThreshold: 0.85  # 85% 차면 오래된 항목 제거
    gcInterval: "30s"        # Garbage Collection 간격
EOF

# 설정 적용
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system --reuse-values \
  -f /tmp/bpf-map-tuning.yaml

# 재시작 및 확인
kubectl -n kube-system rollout restart ds/cilium

# Cilium Status 확인
cilium status --wait 
```

#### Map 압력 테스트

```bash
# 대량 연결 생성 테스트
cat << 'EOF' > /tmp/map-pressure-test.sh
#!/bin/bash

echo "=== eBPF Map Pressure Test ==="

# 테스트 네임스페이스 생성
kubectl create namespace map-test

# 서버 Pod 생성
cat <<YAML | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: echo-server
  namespace: map-test
spec:
  selector:
    app: echo-server
  ports:
  - port: 8080
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo-server
  namespace: map-test
spec:
  replicas: 5
  selector:
    matchLabels:
      app: echo-server
  template:
    metadata:
      labels:
        app: echo-server
    spec:
      containers:
      - name: echo
        image: hashicorp/http-echo:latest
        args: ["-listen=:8080", "-text=echo"]
        ports:
        - containerPort: 8080
YAML

# 클라이언트 Pod 생성 (대량 연결 생성)
cat <<YAML | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: connection-generator
  namespace: map-test
spec:
  parallelism: 10
  completions: 10
  template:
    spec:
      containers:
      - name: client
        image: curlimages/curl:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Generating connections..."
          for i in \$(seq 1 1000); do
            curl -s http://echo-server:8080 > /dev/null &
            if [ \$((i % 100)) -eq 0 ]; then
              echo "Generated \$i connections"
              wait
            fi
          done
          wait
          echo "Completed"
      restartPolicy: Never
YAML

# Map 사용량 모니터링
echo "Monitoring eBPF Map usage..."
for i in {1..10}; do
  echo "=== Iteration $i ==="
  kubectl exec -n kube-system ds/cilium -- cilium bpf ct list global | wc -l
  kubectl exec -n kube-system ds/cilium -- cilium metrics list | grep -E "bpf_map_ops_total|bpf_map_pressure"
  sleep 5
done
EOF

chmod +x /tmp/map-pressure-test.sh
/tmp/map-pressure-test.sh

# 10번 시도한 누적 결과를 보여줌
# cilium_bpf_map_ops_total                                                 map_name=ct4_global operation=delete outcome=success                               362.000000
# cilium_bpf_map_ops_total                                                 map_name=ct_any4_global operation=delete outcome=success                           190.000000
# cilium_bpf_map_ops_total                                                 map_name=ipcache_v2 operation=update outcome=success                               20.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_backends_v3 operation=delete outcome=success                          1.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_backends_v3 operation=update outcome=success                          16.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_reverse_nat operation=update outcome=success                          32.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_services_v2 operation=delete outcome=success                          1.000000
# cilium_bpf_map_ops_total                                                 map_name=lb4_services_v2 operation=update outcome=success                          56.000000
# cilium_bpf_map_ops_total                                                 map_name=lb_affinity_match operation=delete outcome=fail                           24.000000
# cilium_bpf_map_ops_total                                                 map_name=lxc operation=update outcome=success                                      6.000000
# cilium_bpf_map_ops_total                                                 map_name=policy operation=update outcome=success                                   3.000000
# cilium_bpf_map_ops_total                                                 map_name=runtime_config operation=update outcome=success                           10.000000
# cilium_bpf_map_ops_total                                                 map_name=snat_v4_external operation=delete outcome=success                         428.000000
# cilium_bpf_map_pressure                                                  map_name=ct4_global                                                                0.000400
# cilium_bpf_map_pressure                                                  map_name=ct_any4_global                                                            0.001180
# cilium_bpf_map_pressure                                                  map_name=ipcache_v2                                                                0.000039
# cilium_bpf_map_pressure                                                  map_name=lxc                                                                       0.000092

# 정리
kubectl delete namespace map-test
```

#### Map 유형별 최적화

```bash
# Connection Tracking Map 최적화
cat << EOF > /tmp/ct-optimize-values.yaml
bpf:
  # Connection Tracking Map 크기
  ctMapEntriesGlobalTCP: 1000000
  ctMapEntriesGlobalAny: 500000
  
  # TCP Timeout 설정 (초 단위)
  tcpFinTimeout: 10
  tcpCloseTimeout: 10
  tcpTimeWaitTimeout: 10
  tcpConnectionTimeout: 120
  tcpSynTimeout: 60
  
  # CT Garbage Collection
  ctGcInterval: 30s
  ctGcMaxInterval: 5m
EOF

# 설정 적용
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system --reuse-values \
  -f /tmp/ct-optimize-values.yaml

# 재시작 및 확인
kubectl -n kube-system rollout restart ds/cilium

# Cilium Status 확인
cilium status --wait

# Cilium ConfigMap에서 설정 확인
kubectl get configmap -n kube-system cilium-config -o yaml | grep -E "ct-|tcp-"

# auto-direct-node-routes: "true"
# bpf-events-policy-verdict-enabled: "true"
# direct-routing-skip-unreachable: "false"
# enable-auto-protect-node-port-range: "true"
# proxy-connect-timeout: "2"
# tofqdns-dns-reject-response-code: refused

# Helm values 확인
helm get values cilium -n kube-system | grep -A 10 "bpf:"

# bpf:
#   ctGcInterval: 30s
#   ctGcMaxInterval: 5m
#   ctMapEntriesGlobalAny: 500000
#   ctMapEntriesGlobalTCP: 1000000
#   lbMapMax: 65536
#   mapDynamicSizeRatio: 0.0025
#   mapPressure:
#     enabled: true
#     evictionThreshold: 0.85
#     gcInterval: 30s

# CT Map 상태 직접 확인
kubectl exec -n kube-system ds/cilium -- cilium bpf ct list global | head -5

# TCP IN 192.168.97.2:38064 -> 10.244.0.149:8181 expires=94416 Packets=0 Bytes=0 RxFlagsSeen=0x1b LastRxReport=94406 TxFlagsSeen=0x1b LastTxReport=94406 Flags=0x0013 [ RxClosing TxClosing SeenNonSyn ] RevNAT=0 SourceSecurityID=1 BackendID=0 
# TCP IN 192.168.97.2:39004 -> 10.244.0.143:8080 expires=94387 Packets=0 Bytes=0 RxFlagsSeen=0x1b LastRxReport=94377 TxFlagsSeen=0x1b LastTxReport=94377 Flags=0x0013 [ RxClosing TxClosing SeenNonSyn ] RevNAT=0 SourceSecurityID=1 BackendID=0 
# TCP IN 192.168.97.2:47162 -> 10.244.0.105:8181 expires=94411 Packets=0 Bytes=0 RxFlagsSeen=0x1b LastRxReport=94401 TxFlagsSeen=0x1b LastTxReport=94401 Flags=0x0013 [ RxClosing TxClosing SeenNonSyn ] RevNAT=0 SourceSecurityID=1 BackendID=0 
# TCP IN 192.168.97.2:50518 -> 10.244.0.143:8080 expires=94398 Packets=0 Bytes=0 RxFlagsSeen=0x1b LastRxReport=94388 TxFlagsSeen=0x1b LastTxReport=94388 Flags=0x0013 [ RxClosing TxClosing SeenNonSyn ] RevNAT=0 SourceSecurityID=1 BackendID=0 
# TCP OUT 192.168.97.2:53732 -> 10.244.0.12:9090 expires=94406 Packets=0 Bytes=0 RxFlagsSeen=0x00 LastRxReport=0 TxFlagsSeen=0x02 LastTxReport=94346 Flags=0x0000 [ ] RevNAT=0 SourceSecurityID=0 BackendID=0

# CT Map 통계 확인
kubectl exec -n kube-system ds/cilium -- cilium metrics list | grep -E "conntrack|ct_"

# cilium_act_processing_time_seconds                                                                                                                          0s / 0s / 0s
# cilium_bpf_map_capacity                                                  map_group=cilium_ct_any4_global                                                    73729.000000
# cilium_bpf_map_ops_total                                                 map_name=ct_any4_global operation=delete outcome=success                           317.000000
# cilium_bpf_map_pressure                                                  map_name=ct_any4_global                                                            0.001804
# cilium_datapath_conntrack_dump_resets_total                              area=conntrack family=ipv4 name=dump_interrupts                                    0.000000
# cilium_datapath_conntrack_gc_duration_seconds                            family=ipv4 protocol=TCP status=completed                                          2.5ms / 4.5ms / 4.95ms
# cilium_datapath_conntrack_gc_duration_seconds                            family=ipv4 protocol=non-TCP status=completed                                      2.5ms / 4.5ms / 4.95ms
# cilium_datapath_conntrack_gc_entries                                     family=ipv4 protocol=TCP status=alive                                              58.000000
# cilium_datapath_conntrack_gc_entries                                     family=ipv4 protocol=TCP status=deleted                                            1.000000
# cilium_datapath_conntrack_gc_entries                                     family=ipv4 protocol=non-TCP status=alive                                          90.000000
# cilium_datapath_conntrack_gc_entries                                     family=ipv4 protocol=non-TCP status=deleted                                        0.000000
# cilium_datapath_conntrack_gc_interval_seconds                            global=global                                                                      450.000000
# cilium_datapath_conntrack_gc_key_fallbacks_total                         family=ipv4 protocol=TCP                                                           0.000000
# cilium_datapath_conntrack_gc_key_fallbacks_total                         family=ipv4 protocol=non-TCP                                                       0.000000
# cilium_datapath_conntrack_gc_runs_total                                  family=ipv4 protocol=TCP status=completed                                          2.000000
# cilium_datapath_conntrack_gc_runs_total                                  family=ipv4 protocol=non-TCP status=completed                                      2.000000
# cilium_feature_adv_connect_and_lb_bandwidth_manager_enabled                                                                                                 0.000000
# cilium_feature_adv_connect_and_lb_bgp_enabled                                                                                                               0.000000
# cilium_feature_adv_connect_and_lb_cilium_envoy_config_enabled                                                                                               0.000000
# cilium_feature_adv_connect_and_lb_cilium_node_config_enabled                                                                                                0.000000
# cilium_feature_adv_connect_and_lb_egress_gateway_enabled                                                                                                    0.000000
# cilium_feature_adv_connect_and_lb_envoy_proxy_enabled                    mode=standalone                                                                    1.000000
# cilium_feature_adv_connect_and_lb_k8s_internal_traffic_policy_enabled                                                                                       1.000000
# cilium_feature_adv_connect_and_lb_kube_proxy_replacement_enabled                                                                                            1.000000
# cilium_feature_adv_connect_and_lb_l2_lb_enabled                                                                                                             0.000000
# cilium_feature_adv_connect_and_lb_l2_pod_announcement_enabled                                                                                               0.000000
# cilium_feature_adv_connect_and_lb_node_port_configuration                acceleration=disabled algorithm=random mode=snat                                   1.000000
# cilium_feature_adv_connect_and_lb_sctp_enabled                                                                                                              0.000000
# cilium_feature_adv_connect_and_lb_vtep_enabled                                                                                                              0.000000
# cilium_feature_network_policies_local_redirect_policy_enabled                                                                                               0.000000
```

### CiliumEndpointSlice (CES) 심화 실습

대규모 환경에서 Watch 이벤트를 줄이고 성능을 개선하는 CES를 구성하고 테스트합니다.

#### CES 기능 이해 및 활성화

```bash
# 현재 CiliumEndpoint 수 확인
echo "Current CiliumEndpoints count:"
kubectl get ciliumendpoints -A --no-headers | wc -l

# CiliumEndpointSlice 활성화
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system --reuse-values \
  --set ciliumEndpointSlice.enabled=true \
  --set ciliumEndpointSlice.endpointsPerSlice=100

kubectl rollout restart -n kube-system deployment cilium-operator
kubectl rollout restart -n kube-system ds/cilium

# CES 리소스 확인
kubectl get ciliumendpointslices.cilium.io -A
kubectl get ces -A  # 축약형
```

#### CES vs CEP 성능 비교

```bash
# 테스트 네임스페이스 생성
kubectl create namespace ces-test

# 대량 Pod 생성 스크립트
cat << 'EOF' > /tmp/create-pods.sh
#!/bin/bash
NAMESPACE="ces-test"
POD_COUNT=200

echo "Creating $POD_COUNT pods..."
for i in $(seq 1 $POD_COUNT); do
  cat <<PODEOF | kubectl apply -f - &
apiVersion: v1
kind: Pod
metadata:
  name: test-pod-$i
  namespace: $NAMESPACE
  labels:
    app: ces-test
    pod-id: "$i"
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    resources:
      requests:
        cpu: "5m"
        memory: "10Mi"
      limits:
        cpu: "10m"
        memory: "20Mi"
PODEOF
  
  # 병렬 처리 제한
  if [ $((i % 10)) -eq 0 ]; then
    wait
  fi
done
wait
echo "Pod creation completed"
EOF

chmod +x /tmp/create-pods.sh

# CEP 모드에서 Watch 이벤트 측정
echo "=== Testing with CiliumEndpoint (CEP) ==="
kubectl delete namespace ces-test --ignore-not-found
kubectl create namespace ces-test

# Watch 이벤트 수집 시작 (백그라운드)
kubectl get ciliumendpoints -A --watch > /tmp/cep-watch.log 2>&1 &
WATCH_PID=$!

# Pod 생성
time /tmp/create-pods.sh

# Watch 이벤트 수집 중지
sleep 5
kill $WATCH_PID 2>/dev/null

# CEP Watch 이벤트 수 계산
CEP_EVENTS=$(wc -l < /tmp/cep-watch.log)
echo "CEP Watch events: $CEP_EVENTS"

# CES 모드 테스트
echo ""
echo "=== Testing with CiliumEndpointSlice (CES) ==="
kubectl delete namespace ces-test --ignore-not-found
kubectl create namespace ces-test

# Watch 이벤트 수집 시작
kubectl get ces -A --watch > /tmp/ces-watch.log 2>&1 &
WATCH_PID=$!

# Pod 생성
time /tmp/create-pods.sh

# Watch 이벤트 수집 중지
sleep 5
kill $WATCH_PID 2>/dev/null

# CES Watch 이벤트 수 계산
CES_EVENTS=$(wc -l < /tmp/ces-watch.log)
echo "CES Watch events: $CES_EVENTS"

# 비교 결과
echo ""
echo "=== Comparison Results ==="
echo "CEP Watch events: $CEP_EVENTS"
echo "CES Watch events: $CES_EVENTS"
echo "Reduction: $(echo "scale=2; ($CEP_EVENTS - $CES_EVENTS) * 100 / $CEP_EVENTS" | bc)%"

# === Comparison Results ===
# CEP Watch events:       82
# CES Watch events:        5
# Reduction: 93.90%
```

#### CES 상세 분석

```bash
# CES 구조 확인
echo "=== CiliumEndpointSlice Structure ==="
kubectl get ces -n ces-test -o yaml | head -100

# CES 당 엔드포인트 수 확인
for ces in $(kubectl get ces -n ces-test -o name); do
  endpoints=$(kubectl get $ces -n ces-test -o json | jq '.endpoints | length')
  echo "$ces: $endpoints endpoints"
done

# ciliumendpointslice.cilium.io/ces-dtvcfjnvm-cxpry: 2 endpoints
# ciliumendpointslice.cilium.io/ces-jp66t54yg-894zc: 3 endpoints
# ciliumendpointslice.cilium.io/ces-jxdwlzd9w-jpndk: 100 endpoints
# ciliumendpointslice.cilium.io/ces-swk9882dr-4qkz9: 82 endpoints

# API Server 부하 비교
cat << 'EOF' > /tmp/measure-api-load.sh
#!/bin/bash

echo "Measuring API server load..."

# 초기 메트릭 수집
START_TIME=$(date +%s)
kubectl top nodes > /tmp/before-load.txt

# 100개 Pod 동시 업데이트
for i in $(seq 1 100); do
  kubectl label pod test-pod-$i -n ces-test updated=true --overwrite &
done
wait

# 종료 메트릭 수집
END_TIME=$(date +%s)
kubectl top nodes > /tmp/after-load.txt

echo "Update duration: $((END_TIME - START_TIME)) seconds"
echo "Node resource usage change:"
diff /tmp/before-load.txt /tmp/after-load.txt
EOF

chmod +x /tmp/measure-api-load.sh
/tmp/measure-api-load.sh

# 정리
kubectl delete namespace ces-test
```

#### CES 고급 설정

```bash
# CES 세부 튜닝
cat << EOF > /tmp/ces-tuning.yaml
ciliumEndpointSlice:
  enabled: true
  # 슬라이스당 최대 엔드포인트 수
  endpointsPerSlice: 100
  # 레이블 셀렉터 기반 슬라이싱
  enableLabelSelector: true
  # 슬라이스 업데이트 배치 크기
  updateBatchSize: 50
  # 슬라이스 업데이트 간격
  updateInterval: "500ms"

# CEP 백워드 호환성
ciliumEndpoint:
  # CEP 유지 (마이그레이션 기간 동안)
  keepDeprecated: true
EOF

# 설정 적용
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system --reuse-values \
  -f /tmp/ces-tuning.yaml

# 재시작 및 확인
kubectl -n kube-system rollout restart ds/cilium

# Cilium Status 확인
cilium status --wait

# 마이그레이션 상태 확인
kubectl get cep -A --no-headers | wc -l
kubectl get ces -A --no-headers | wc -l 
```

### BIG TCP 활성화 및 성능 테스트

네트워크 처리량 향상을 위해 BIG TCP를 활성화하고 성능을 측정합니다. 
BIG TCP는 GSO(Generic Segmentation Offload)/GRO(Generic Receive Offload) 크기를 최대 192KB까지 증가시켜 패킷 처리 오버헤드를 획기적으로 줄입니다.

#### BIG TCP 이해하기

BIG TCP는 Linux 커널 5.19+에서 도입된 기능으로, 기존 64KB 제한을 넘어 최대 512KB까지 패킷을 처리할 수 있게 합니다.

```bash
# 현재 시스템의 GSO/GRO 제한 확인
ip -d link show | grep -E "gso_max|gro_max"

# 커널 버전 확인 (5.19 이상 필요)
uname -r

# NIC 드라이버 확인 (mlx4, mlx5 권장)
ethtool -i $(ip route | grep default | awk '{print $5}') | grep driver
```

#### BIG TCP 기능 활성화

```bash
# Dual-stack 클러스터 생성 (BIG TCP 최적 환경)
cat <<EOF > /tmp/kind-bigtcp-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
networking:
  ipFamily: dual
  disableDefaultCNI: true
  podSubnet: "10.244.0.0/16,fd00:10:244::/56"
  serviceSubnet: "10.96.0.0/16,fd00:10:96::/112"
EOF

# 기존 클러스터 삭제 후 재생성 (선택사항)
# kind delete cluster --name myk8s
# kind create cluster --name myk8s --config /tmp/kind-bigtcp-config.yaml

# Cilium을 BIG TCP 지원과 함께 설치
cilium install --version 1.18.1 \
  --set ipam.mode=kubernetes \
  --set ipv4NativeRoutingCIDR=10.244.0.0/16 \
  --set enableIPv4BIGTCP=true \
  --set enableIPv6BIGTCP=true \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  --set endpointRoutes.enabled=true \
  --set kubeProxyReplacement=true \
  --set bpf.masquerade=true

# BIG TCP 활성화 확인
kubectl exec -it -n kube-system ds/cilium -- cilium status | grep -i big

# IPv4 BIG TCP:            Enabled   [196608]
# IPv6 BIG TCP:            Enabled   [196608]

# 노드의 GSO/GRO 크기 확인 (192KB로 자동 설정됨)
docker exec myk8s-control-plane ip -d link show | grep -E "gso_max_size|gro_max_size"
```

#### BIG TCP 성능 벤치마크

```bash
# 테스트용 네임스페이스 생성
kubectl create namespace bigtcp-test

# 네트워크 성능 테스트 Pod 배포 (netperf 기반)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: iperf3-server
  namespace: bigtcp-test
spec:
  selector:
    app: iperf3-server
  ports:
  - port: 5201
    targetPort: 5201
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iperf3-server
  namespace: bigtcp-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: iperf3-server
  template:
    metadata:
      labels:
        app: iperf3-server
    spec:
      containers:
      - name: iperf3
        image: networkstatic/iperf3:latest
        command: ["iperf3", "-s"]
        ports:
        - containerPort: 5201
---
apiVersion: v1
kind: Pod
metadata:
  name: iperf3-client
  namespace: bigtcp-test
spec:
  containers:
  - name: iperf3
    image: networkstatic/iperf3:latest
    command: ["sleep", "infinity"]
EOF

# Pod 준비 대기
kubectl wait --for=condition=ready pod -l app=iperf3-server -n bigtcp-test --timeout=60s
kubectl wait --for=condition=ready pod/iperf3-client -n bigtcp-test --timeout=60s

# 1. 표준 TCP (작은 MSS)
echo "1. Standard TCP (MSS=1460):"
kubectl exec -n bigtcp-test iperf3-client -- \
  iperf3 -c iperf3-server -t 5 -M 1460 | tail -5

echo ""

# 2. Jumbo Frame 크기
echo "2. Jumbo Frame Size (MSS=9000):"
kubectl exec -n bigtcp-test iperf3-client -- \
  iperf3 -c iperf3-server -t 5 -M 9000 | tail -5

echo ""

# 3. BIG TCP (큰 MSS)
echo "3. BIG TCP (MSS=65495):"
kubectl exec -n bigtcp-test iperf3-client -- \
  iperf3 -c iperf3-server -t 5 -M 65495 | tail -5

# 성능 비교 요약
echo ""
echo "=== Performance Summary ==="
for mss in 1460 9000 65495; do
  result=$(kubectl exec -n bigtcp-test iperf3-client -- \
    iperf3 -c iperf3-server -t 3 -M $mss 2>/dev/null | \
    grep sender | awk '{print $7, $8}')
  echo "MSS=$mss: $result"
done

# iperf3 기반 상세 테스트
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: iperf3-server
  namespace: bigtcp-test
spec:
  selector:
    app: iperf3-server
  ports:
  - port: 5201
    targetPort: 5201
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iperf3-server
  namespace: bigtcp-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: iperf3-server
  template:
    metadata:
      labels:
        app: iperf3-server
    spec:
      containers:
      - name: iperf3
        image: networkstatic/iperf3:latest
        command: ["iperf3", "-s"]
        ports:
        - containerPort: 5201
---
apiVersion: v1
kind: Pod
metadata:
  name: iperf3-client
  namespace: bigtcp-test
spec:
  containers:
  - name: iperf3
    image: networkstatic/iperf3:latest
    command: ["sleep", "infinity"]
EOF

# iperf3 Pod 대기
kubectl wait --for=condition=ready pod -l app=iperf3-server -n bigtcp-test --timeout=60s

# MSS 크기별 성능 테스트
echo ""
echo "=== iperf3 MSS Size Comparison ==="
for mss in 1460 9000 65495; do
  echo "Testing with MSS=$mss bytes:"
  kubectl exec -n bigtcp-test iperf3-client -- \
    iperf3 -c iperf3-server -t 5 -M $mss 2>/dev/null | grep sender | tail -1
done
```

---

## ClusterLoader2 활용

### CL2 설치 및 구성

Kubernetes 공식 성능 테스트 프레임워크인 ClusterLoader2를 설치하고 구성합니다.

```bash
# Go 설치 확인 (1.19 이상 필요)
go version

# perf-tests 저장소 클론
git clone https://github.com/kubernetes/perf-tests.git
cd perf-tests/clusterloader2

# 의존성 설치
go mod download

# ClusterLoader2 빌드
go build -o clusterloader cmd/clusterloader.go
```

### Pod 시작 지연시간 측정

Pod 생성부터 Running 상태까지의 시간을 측정합니다.

```bash
# 테스트 디렉토리 생성
mkdir -p testing/pod-startup
cd testing/pod-startup

# 테스트 설정 파일
cat << EOF > pod-startup-config.yaml
name: pod-startup-latency-test
namespace:
  number: 1
  prefix: cl2-test
  
tuningSets:
- name: Uniform5qps
  qpsLoad:
    qps: 5
    burst: 10

steps:
- name: Start pod startup latency measurement
  measurements:
  - Identifier: PodStartupLatency
    Method: PodStartupLatency
    Params:
      action: start
      labelSelector: app = cl2-test-pod
      threshold: 30s

- name: Create pods with varying sizes
  phases:
  - namespaceRange:
      min: 1
      max: 1
    replicasPerNamespace: 50
    tuningSet: Uniform5qps
    objectBundle:
    - basename: test-pod
      objectTemplatePath: "pod.yaml"
      templateFillMap:
        Image: nginx:alpine

- name: Wait for pods to be running
  measurements:
  - Identifier: WaitForRunningPods
    Method: WaitForRunningPods
    Params:
      action: start
      labelSelector: app = cl2-test-pod
      desiredPodCount: 50
      timeout: 5m

- name: Gather pod startup metrics
  measurements:
  - Identifier: PodStartupLatency
    Method: PodStartupLatency
    Params:
      action: gather
  - Identifier: WaitForRunningPods
    Method: WaitForRunningPods
    Params:
      action: gather
EOF

# Pod 템플릿
cat << EOF > pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: {{.Name}}
  labels:
    app: cl2-test-pod
spec:
  containers:
  - name: container
    image: {{.Image}}
    resources:
      requests:
        cpu: "10m"
        memory: "20Mi"
      limits:
        cpu: "50m"
        memory: "50Mi"
EOF

# 테스트 실행
../../clusterloader \
  --testconfig=pod-startup-config.yaml \
  --provider=kind \
  --kubeconfig=${HOME}/.kube/config \
  --v=2 \
  --report-dir=./reports

# E0831 02:48:37.189555   26153 pod_startup_latency.go:276] StatelessPodStartupLatency: labelSelector(app = cl2-test-pod): pod startup: too high latency 90th percentile: got 45.532733s expected: 30s
```

### 대규모 Deployment 성능 테스트

수백 개의 Pod를 관리하는 Deployment의 성능을 측정합니다.

```bash
# 대규모 Deployment 테스트 설정
cat << EOF > large-deployment-config.yaml
name: large-deployment-test
namespace:
  number: 5
  prefix: cl2-deploy

tuningSets:
- name: FastScheduling
  qpsLoad:
    qps: 50
    burst: 100

measurements:
- name: deploymentMetrics

steps:
- name: Start measurements
  measurements:
  - Identifier: APIResponsiveness
    Method: APIResponsiveness
    Params:
      action: start
  - Identifier: SchedulingThroughput
    Method: SchedulingThroughput
    Params:
      action: start
      labelSelector: app = large-app

- name: Create large deployments
  phases:
  - namespaceRange:
      min: 1
      max: 5
    replicasPerNamespace: 3
    tuningSet: FastScheduling
    objectBundle:
    - basename: large-deployment
      objectTemplatePath: "deployment.yaml"
      templateFillMap:
        Replicas: 100
        Image: registry.k8s.io/pause:3.9

- name: Scale deployments
  phases:
  - namespaceRange:
      min: 1
      max: 5
    replicasPerNamespace: 3
    tuningSet: FastScheduling
    objectBundle:
    - basename: large-deployment
      objectTemplatePath: "deployment-scale.yaml"
      templateFillMap:
        Replicas: 200

- name: Gather metrics
  measurements:
  - Identifier: APIResponsiveness
    Method: APIResponsiveness
    Params:
      action: gather
  - Identifier: SchedulingThroughput
    Method: SchedulingThroughput
    Params:
      action: gather
      
- name: Delete deployments
  phases:
  - namespaceRange:
      min: 1
      max: 5
    replicasPerNamespace: 0
    tuningSet: FastScheduling
    objectBundle:
    - basename: large-deployment
      objectTemplatePath: "deployment.yaml"
EOF

# Deployment 템플릿
cat << EOF > deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{.Name}}
  labels:
    app: large-app
spec:
  replicas: {{.Replicas}}
  selector:
    matchLabels:
      app: {{.Name}}
  template:
    metadata:
      labels:
        app: {{.Name}}
    spec:
      containers:
      - name: pause
        image: {{.Image}}
        resources:
          requests:
            cpu: "5m"
            memory: "10Mi"
EOF

# Scale 템플릿
cat << EOF > deployment-scale.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{.Name}}
spec:
  replicas: {{.Replicas}}
EOF

# 테스트 실행
../../clusterloader \
  --testconfig=large-deployment-config.yaml \
  --provider=kind \
  --kubeconfig=${HOME}/.kube/config \
  --v=2 \
  --report-dir=./reports \
  --enable-prometheus-server
```

### API Server 부하 테스트

API Server의 처리 능력을 측정하는 집중 부하 테스트입니다.

```bash
# API 부하 테스트 설정
cat << EOF > api-load-config.yaml
name: api-server-load-test
namespace:
  number: 10
  prefix: api-load

tuningSets:
- name: HighQPS
  qpsLoad:
    qps: 100
    burst: 200
- name: BurstLoad
  randomizedLoad:
    averageQps: 50
    minQps: 10
    maxQps: 200
    burstDuration: 30s

steps:
- name: Start API responsiveness measurement
  measurements:
  - Identifier: APIResponsiveness
    Method: APIResponsiveness
    Params:
      action: start
      enableViolations: true
      useSimpleLatencyQuery: false
      summaryPrintCount: 10

- name: Create ConfigMaps (Read heavy)
  phases:
  - namespaceRange:
      min: 1
      max: 10
    replicasPerNamespace: 100
    tuningSet: HighQPS
    objectBundle:
    - basename: configmap
      objectTemplatePath: "configmap.yaml"

- name: List operations load
  phases:
  - namespaceRange:
      min: 1
      max: 10
    replicasPerNamespace: 1
    tuningSet: BurstLoad
    objectBundle:
    - basename: list-job
      objectTemplatePath: "list-job.yaml"
      listUnknownObjectOptions:
        labelSelector: type=configmap

- name: Update ConfigMaps (Write heavy)
  phases:
  - namespaceRange:
      min: 1
      max: 10
    replicasPerNamespace: 100
    tuningSet: HighQPS
    objectBundle:
    - basename: configmap
      objectTemplatePath: "configmap-update.yaml"

- name: Gather API metrics
  measurements:
  - Identifier: APIResponsiveness
    Method: APIResponsiveness
    Params:
      action: gather
      percentiles: [50, 90, 95, 99, 99.9]
EOF

# ConfigMap 템플릿
cat << EOF > configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Name}}
  labels:
    type: configmap
data:
  key1: "value1"
  key2: "value2"
  timestamp: "{{.CurrentTimestamp}}"
EOF

# List Job 템플릿
cat << EOF > list-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: {{.Name}}
spec:
  template:
    spec:
      containers:
      - name: kubectl
        image: bitnami/kubectl:latest
        command: 
        - /bin/sh
        - -c
        - |
          for i in \$(seq 1 100); do
            kubectl get configmaps -n {{.Namespace}} -l type=configmap
            sleep 0.1
          done
      restartPolicy: Never
  backoffLimit: 1
EOF

# ConfigMap Update 템플릿
cat << EOF > configmap-update.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Name}}
data:
  key1: "updated-value1"
  key2: "updated-value2"
  updateTime: "{{.CurrentTimestamp}}"
EOF

# 테스트 실행
../../clusterloader \
  --testconfig=api-load-config.yaml \
  --provider=kind \
  --kubeconfig=${HOME}/.kube/config \
  --v=2 \
  --report-dir=./reports
```

### ClusterLoader2 Report 결과 분석

테스트 완료 후 생성된 리포트를 분석합니다.

```bash
# 리포트 디렉토리 확인
ls -la ./reports/

# JSON 형식 결과 파싱
cat ./reports/PodStartupLatency_*.json | jq '.dataItems[].data.Perc99'

# Prometheus 메트릭 확인 (프로메테우스 서버 활성화 시)
cat ./reports/prometheus_metrics.txt | grep -E "apiserver_request_duration|scheduler_scheduling_duration"

# HTML 리포트 생성
cat << 'EOF' > generate-report.sh
#!/bin/bash
REPORT_DIR="./reports"
OUTPUT_FILE="cl2-report.html"

echo "<html><head><title>CL2 Test Report</title></head><body>" > $OUTPUT_FILE
echo "<h1>ClusterLoader2 Performance Test Results</h1>" >> $OUTPUT_FILE

for json in $REPORT_DIR/*.json; do
  testname=$(basename $json .json)
  echo "<h2>$testname</h2>" >> $OUTPUT_FILE
  echo "<pre>" >> $OUTPUT_FILE
  cat $json | jq '.' >> $OUTPUT_FILE
  echo "</pre>" >> $OUTPUT_FILE
done

echo "</body></html>" >> $OUTPUT_FILE
echo "Report generated: $OUTPUT_FILE"
EOF

chmod +x generate-report.sh
./generate-report.sh
```

---
