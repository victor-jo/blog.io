---
layout: post
title: "Cilium 네트워킹 핸즈온 가이드: Service, LB-IPAM, L2 Announcements"
date: 2025-08-09 10:00:00 +0900
categories: cilium kubernetes
tags: [cilium, kubernetes, service, lb-ipam, l2-announcements, loadbalancer, networking, ebpf, vxlan]
---

## Cilium 네트워킹 핸즈온 가이드 (2)

최근 클라우드 네이티브 환경에서 Cilium이 제공하는 고급 네트워킹 기능들이 주목받고 있습니다. 특히 Service LoadBalancer와 L2 Announcements는 온프레미스 환경에서 MetalLB를 대체할 수 있는 강력한 대안으로 부상하고 있습니다. 이번 포스트에서는 Cilium의 핵심 기능들을 실습을 통해 깊이 있게 다뤄보겠습니다.

## Native Routing vs Overlay Network

### Native Routing 모드의 한계

Native Routing은 캡슐화 오버헤드가 없어 성능이 우수하지만, 서로 다른 서브넷 간 통신에는 별도 라우팅 설정이 필요합니다.

#### 샘플 애플리케이션 배포
```bash
# webpod 애플리케이션 배포 (Anti-Affinity로 노드 분산)
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: webpod
            topologyKey: kubernetes.io/hostname
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod  
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

# k8s-ctr 노드에 curl-pod 파드 배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF
```

#### 통신 문제 확인
```bash
# k8s-w0 노드의 파드 IP 확인
WEBPOD=$(kubectl get pod -l app=webpod \
  --field-selector spec.nodeName=k8s-w0 \
  -o jsonpath='{.items[0].status.podIP}')
echo "k8s-w0 Pod IP: $WEBPOD"

# 터미널1 (router)
tcpdump -i any icmp -nn

# router 에서는 172 대역이 없음
# default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100
# 10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100
# 10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100
# 10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100
# 10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200
# 10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200
# 192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200
# 192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200

# 터미널2 (k8s-ctr) - 통신 실패 확인
kubectl exec -it curl-pod -- ping -c 2 $WEBPOD

# tcpdump 결과 분석:
# 22:20:27.654000 eth1  In  IP 172.20.0.124 > 172.20.2.178: ICMP echo request, id 31, seq 1, length 64
# 22:20:27.654011 eth0  Out IP 172.20.0.124 > 172.20.2.178: ICMP echo request, id 31, seq 1, length 64
# 22:20:28.711521 eth1  In  IP 172.20.0.124 > 172.20.2.178: ICMP echo request, id 31, seq 2, length 64
# 22:20:28.711532 eth0  Out IP 172.20.0.124 > 172.20.2.178: ICMP echo request, id 31, seq 2, length 64
# → eth0(10.0.2.15)로 잘못 라우팅됨

```

#### 임시 대책 처리
```bash
# Router에 Static Route 추가
sshpass -p 'vagrant' ssh -T vagrant@router << 'EOF'
# k8s-ctr의 Pod CIDR 라우팅
sudo ip route add 172.20.0.0/24 via 192.168.10.100

# k8s-w1의 Pod CIDR 라우팅
sudo ip route add 172.20.1.0/24 via 192.168.10.101

# k8s-w0의 Pod CIDR 라우팅  
sudo ip route add 172.20.2.0/24 via 192.168.20.100

# 라우팅 테이블 확인
ip route | grep 172.20
EOF
```

#### 임시 대책 후 통신 상태 확인
```bash
# k8s-w0 노드의 파드 IP 확인
WEBPOD=$(kubectl get pod -l app=webpod \
  --field-selector spec.nodeName=k8s-w0 \
  -o jsonpath='{.items[0].status.podIP}')
echo "k8s-w0 Pod IP: $WEBPOD"

# 터미널2 (k8s-ctr) - 통신 성공 확인
kubectl exec -it curl-pod -- ping -c 2 $WEBPOD

# 이더넷 2개에서 들어왔다가 나갔다가 응답됨
# 23:34:33.877801 eth1  In  IP 172.20.0.204 > 172.20.2.6: ICMP echo request, id 24, seq 1, length 64
# 23:34:33.877814 eth2  Out IP 172.20.0.204 > 172.20.2.6: ICMP echo request, id 24, seq 1, length 64
# 23:34:33.878349 eth2  In  IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 24, seq 1, length 64
# 23:34:33.878353 eth1  Out IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 24, seq 1, length 64
# 23:34:34.878344 eth1  In  IP 172.20.0.204 > 172.20.2.6: ICMP echo request, id 24, seq 2, length 64
# 23:34:34.878356 eth2  Out IP 172.20.0.204 > 172.20.2.6: ICMP echo request, id 24, seq 2, length 64
# 23:34:34.878789 eth2  In  IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 24, seq 2, length 64
# 23:34:34.878793 eth1  Out IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 24, seq 2, length 64
```

#### 왜 임시 대책인가?

노드가 100대 이상? Pod CIDR 이 변경됨? 그러한 상황에서 '노가다'가 굉장히 필요합니다. <br/>
일일이 다해줘야..? 심각...

#### 임시 대책 제거
```bash
# Router에서 추가한 라우트 삭제
sshpass -p 'vagrant' ssh vagrant@router << 'EOF'
sudo ip route del 172.20.0.0/24
sudo ip route del 172.20.1.0/24
sudo ip route del 172.20.2.0/24

# 삭제 확인
ip route | grep 172.20
EOF
```

### VXLAN Overlay Network로 해결

Overlay Network는 50바이트의 헤더 오버헤드가 있지만, 서브넷 간 통신을 자동으로 처리합니다.

#### VXLAN 모드 전환
```bash
# 커널 모듈 로드
modprobe vxlan
lsmod | grep vxlan
for i in w1 w0; do 
  sshpass -p 'vagrant' ssh vagrant@k8s-$i sudo modprobe vxlan
  sshpass -p 'vagrant' ssh vagrant@k8s-$i lsmod | grep vxlan
done

# Cilium을 VXLAN 모드로 전환
helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --version 1.18.0 \
  --reuse-values \
  --set routingMode=tunnel \
  --set tunnelProtocol=vxlan \
  --set autoDirectNodeRoutes=false \
  --set installNoConntrackIptablesRules=false

kubectl rollout restart deploy cilium-operator -n kube-system
kubectl rollout restart ds cilium -n kube-system
```

#### 통신 검증
```bash
# VXLAN 인터페이스 확인
ip link show cilium_vxlan
# cilium_vxlan: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500

# MTU 1450 확인 (VXLAN 오버헤드로 인한 감소)
ip route | grep cilium_host

# 172.20.0.0/24 via 172.20.0.136 dev cilium_host proto kernel src 172.20.0.136
# 172.20.0.136 dev cilium_host proto kernel scope link
# 172.20.1.0/24 via 172.20.0.136 dev cilium_host proto kernel src 172.20.0.136 mtu 1450
# 172.20.2.0/24 via 172.20.0.136 dev cilium_host proto kernel src 172.20.0.136 mtu 1450

# 통신 성공 확인
kubectl exec -it curl-pod -- curl webpod | grep Hostname
# Hostname: webpod-697b545f57-xxxxx

# Hubble로 overlay 통신 확인
cilium hubble port-forward&

hubble observe -f --protocol tcp --pod curl-pod

# Aug  9 14:06:08.749: default/curl-pod:40372 (ID:14553) -> default/webpod-697b545f57-lq7nw:80 (ID:8246) to-overlay FORWARDED (TCP Flags: ACK, FIN)


# k8s-w0 노드의 파드 IP 확인
WEBPOD=$(kubectl get pod -l app=webpod \
  --field-selector spec.nodeName=k8s-w0 \
  -o jsonpath='{.items[0].status.podIP}')
echo "k8s-w0 Pod IP: $WEBPOD"


# k8s-w0 에서 tcpdump
tcpdump -i any icmp -nn

# cilium_vxlan 을 거쳐 통신 성공 확인
kubectl exec -it curl-pod -- ping -c 2 $WEBPOD

# 23:47:33.310117 cilium_vxlan P   IP 172.20.0.204 > 172.20.2.6: ICMP echo request, id 71, seq 1, length 64
# 23:47:33.310240 lxca2047e9fb417 In  IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 71, seq 1, length 64
# 23:47:33.310265 cilium_vxlan Out IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 71, seq 1, length 64
# 23:47:34.311252 cilium_vxlan P   IP 172.20.0.204 > 172.20.2.6: ICMP echo request, id 71, seq 2, length 64
# 23:47:34.311316 lxca2047e9fb417 In  IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 71, seq 2, length 64
# 23:47:34.311322 cilium_vxlan Out IP 172.20.2.6 > 172.20.0.204: ICMP echo reply, id 71, seq 2, length 64
```

---

## Service LoadBalancer와 LB-IPAM

### LB-IPAM 소개

LoadBalancer IP Address Management(LB-IPAM)는 클라우드 프로바이더 없이도 LoadBalancer 타입 서비스에 External IP를 자동 할당하는 기능입니다.

#### IP Pool 생성
```bash
# LoadBalancer IP Pool 정의
cat <<EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumLoadBalancerIPPool
metadata:
  name: cilium-lb-ippool
spec:
  blocks:
  - start: "192.168.10.241"
    stop: "192.168.10.245"
EOF

# Pool 상태 확인
kubectl get ippools

# NAME               DISABLED   CONFLICTING   IPS AVAILABLE   AGE
# cilium-lb-ippool   false      False         5               14s
```

#### LoadBalancer 서비스 생성
```bash
# webpod 서비스를 LoadBalancer로 변경
kubectl patch svc webpod -p '{"spec":{"type":"LoadBalancer"}}'

# External IP 할당 확인
kubectl get svc webpod

# NAME     TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE
# webpod   LoadBalancer   10.96.41.64   192.168.10.241   80:32353/TCP   46s

# LoadBalancer IP로 접근
LBIP=$(kubectl get svc webpod -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl -s $LBIP | grep Hostname

# 로드밸런싱 확인
for i in {1..10}; do 
  curl -s $LBIP | grep Hostname
done | sort | uniq -c
```

### Service 고급 기능

#### 특정 IP 요청
```bash
# Service에 특정 IP 할당
kubectl annotate svc webpod \
  "lbipam.cilium.io/ips=192.168.10.245" --overwrite

kubectl get svc webpod
# EXTERNAL-IP가 192.168.10.245로 변경됨
# NAME     TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE
# webpod   LoadBalancer   10.96.41.64   192.168.10.245   80:32353/TCP   116s
```

#### IP 공유 (Sharing Keys)
```bash
# 같은 IP로 다른 포트 사용
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: webpod-alt
  annotations:
    "lbipam.cilium.io/ips": "192.168.10.245"
    "lbipam.cilium.io/sharing-key": "shared-1"
spec:
  type: LoadBalancer
  selector:
    app: webpod
  ports:
  - port: 8080
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  annotations:
    "lbipam.cilium.io/ips": "192.168.10.245"
    "lbipam.cilium.io/sharing-key": "shared-1"
spec:
  type: LoadBalancer
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
EOF

# 같은 IP, 다른 포트 확인
kubectl get svc | grep webpod
# webpod       LoadBalancer   10.96.41.64   192.168.10.245   80:32353/TCP     2m18s
# webpod-alt   LoadBalancer   10.96.96.76   192.168.10.245   8080:31665/TCP   4s
```

---

## L2 Announcements 구현

### L2 Announcements 활성화

L2 Announcements는 ARP를 통해 LoadBalancer IP를 외부에 광고하여, 클러스터 외부에서 직접 접근 가능하게 합니다.

```bash
# L2 Announcements 기능 활성화
helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --version 1.18.0 \
  --reuse-values \
  --set kubeProxyReplacement=true \
  --set l2announcements.enabled=true \
  --set l2NeighDiscovery.enabled=true

kubectl rollout restart deploy cilium-operator -n kube-system
kubectl rollout restart ds cilium -n kube-system

# 활성화 확인
cilium config view | grep -E 'enable-l2|replacement'

# enable-l2-announcements                           true
# enable-l2-neigh-discovery                         true
# kube-proxy-replacement                            true

# Cilium 상태 확인
cilium status --wait
```

### L2 정책 생성

```bash
# L2 Announcement 정책 (k8s-ctr, k8s-w1 에다가 설정)
cat <<EOF | kubectl apply -f -
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: policy1
spec:
  serviceSelector:
    matchLabels:
      app: webpod
  nodeSelector:
    matchExpressions:
    - key: kubernetes.io/hostname
      operator: In
      values:
      - k8s-w1
      - k8s-ctr
  interfaces:
  - ^eth[1-9]+
  loadBalancerIPs: true
EOF
```

### Leader Election 확인

```bash
# 리더 노드 확인
kubectl -n kube-system get lease
kubectl -n kube-system get lease | grep l2announce

# cilium-l2announce-default-webpod       k8s-w1

# 리더 노드 상세 정보
kubectl -n kube-system get lease/cilium-l2announce-default-webpod -o yaml | \
  grep holderIdentity
# holderIdentity: k8s-w1

# L2 announcement 상태 확인
CILIUMPOD=$(kubectl get pod -n kube-system -l k8s-app=cilium \
  --field-selector spec.nodeName=k8s-ctr \
  -o jsonpath='{.items[0].metadata.name}')

kubectl exec -n kube-system $CILIUMPOD -- \
  cilium-dbg shell -- db/show l2-announce

# IP               NetworkInterface
# 192.168.10.241   eth1
```

### 외부 접근 테스트

```bash
# Router에서 LoadBalancer IP로 접근 (새 터미널)
vagrant ssh router

# ARP 확인
arping -I eth1 192.168.10.241 -c 2

# ARPING 192.168.10.241
# 60 bytes from 08:00:27:85:32:ca (192.168.10.241): index=0 time=162.708 usec
# 60 bytes from 08:00:27:85:32:ca (192.168.10.241): index=1 time=257.708 usec

# HTTP 접근
curl -s 192.168.10.241 | grep Hostname

# Hostname: webpod-65794679cc-kbsc2

# ARP 테이블 확인
arp -a | grep 192.168.10.241

# ? (192.168.10.241) at 08:00:27:85:32:ca [ether] on eth1
```

### Failover 테스트

```bash
# 현재 리더 확인
kubectl -n kube-system get lease | grep l2announce
# cilium-l2announce-default-webpod   k8s-w1

# 리더 노드 재부팅
vagrant ssh k8s-w1
sudo reboot

# 새 리더 선출 확인 (약 15초 소요)
kubectl -n kube-system get lease | grep l2announce
# cilium-l2announce-default-webpod   k8s-ctr

# 서비스 계속 접근 가능
curl -s 192.168.10.241
```

---
