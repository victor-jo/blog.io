---
layout: post
title: "Cilium ì´ë€ ë¬´ì—‡ì¸ê°€? Cilium ê³¼ Flannel ì˜ ë¹„êµ"
date: 2025-01-16 14:30:00 +0900
categories: cilium study
tags: [cilium, victor, chris, kubernetes, cni, docker, what, flannel, containerd, vagrant]
---


### ì‹¤ìŠµ í™˜ê²½ì„¤ì • (macOS)
#### Brew ì„¤ì¹˜
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

#### VirtualBox ì„¤ì¹˜
```bash
brew install --cask virtualbox
```

#### Vagrant ì„¤ì¹˜
```bash
brew install --cask vagrant
```

#### ì‹¤ìŠµ êµ¬ì„± íŒŒì¼ë“¤ ì‹¤í–‰
```bash
mkdir cilium-study
cd cilium-study

curl -O https://raw.githubusercontent.com/victor-jo/blog.io/main/assets/cilium/init_cfg.sh
curl -O https://raw.githubusercontent.com/victor-jo/blog.io/main/assets/cilium/Vagrantfile 
curl -O https://raw.githubusercontent.com/victor-jo/blog.io/main/assets/cilium/k8s-ctr.sh
curl -O https://raw.githubusercontent.com/victor-jo/blog.io/main/assets/cilium/k8s-w.sh

vagrant up
```

#### ì¿ ë²„ë„¤í‹°ìŠ¤ ë…¸ë“œê°€ ì‚¬ìš©í•˜ëŠ” ì´ë”ë„· ì„¤ì •
ì¿ ë²„ë„¤í‹°ìŠ¤ ë…¸ë“œë“¤ì´ Vagrant ë¡œ í”„ë¡œë¹„ì €ë‹ ë˜ì—ˆê¸° ë•Œë¬¸ì— ê¸°ë³¸ê°’ìœ¼ë¡œ ë…¸ë“œë“¤ì˜ eth0 IPë¡œ ì„¤ì •ë©ë‹ˆë‹¤. <br/> ë”°ë¼ì„œ, ì‚¬ìš©í•  ì´ë”ë„· ì–´ëŒ‘í„°ì— ë§ëŠ” IPë¥¼ ì„¤ì •í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
```bash
# ë…¸ë“œë“¤ì˜ eth1 IP í™•ì¸
for i in ctr w1 w2 ; do 
  echo ">> node : k8s-$i <<"
  vagrant ssh k8s-$i -c 'ip -c -4 addr show dev eth1'
  echo
done

# SSH ì ‘ì†
vagrant ssh k8s-ctr   # Control Plane ë…¸ë“œ
vagrant ssh k8s-w1    # Worker ë…¸ë“œ 1
vagrant ssh k8s-w2    # Worker ë…¸ë“œ 2

# ê° ë…¸ë“œì— ì ‘ì†í•´ì„œ Kubelet ì´ ì‚¬ìš©í•˜ëŠ” Node IP ë¥¼ eth1 IP ë¡œ ì„¤ì •
NODEIP=$(ip -4 addr show eth1 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
sed -i "s/^\(KUBELET_KUBEADM_ARGS=\"\)/\1--node-ip=${NODEIP} /" /var/lib/kubelet/kubeadm-flags.env
systemctl daemon-reexec && systemctl restart kubelet

# eth1 IP ë¡œ ì „ì²´ ì„¤ì • í›„ ë…¸ë“œ Internal IP í™•ì¸
k get nodes -owide
```

![Kubernetes Node Information](/assets/cilium//images/k-get-nodes.png)

#### kubeadm init / join ê³¼ì •ì—ì„œ Config í•˜ê¸°
```bash

# ì„¤ì • íŒŒì¼ë“¤
curl -O https://raw.githubusercontent.com/victor-jo/blog.io/main/assets/cilium/kubeadm-init-config.yaml
curl -O https://raw.githubusercontent.com/victor-jo/blog.io/main/assets/cilium/kubeadm-join-config.yaml

# ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
vagrant destroy -f && rm -rf .vagrant/

# ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í›„ ê·¸ëŒ€ë¡œ Run
vagrant up
```

### ğŸŒ Flannel CNI ì†Œê°œ ë° ì„¤ì¹˜

#### Flannel CNI ì•„í‚¤í…ì²˜

![Flannel Architecture](/assets/cilium/images/kube-network-model-vxlan.png)

#### ì„¤ì¹˜ ì „ í™•ì¸
```bash
# í´ëŸ¬ìŠ¤í„° CIDR í™•ì¸
kubectl cluster-info dump | grep -m 2 -E "cluster-cidr|service-cluster-ip-range"

# CoreDNS ìƒíƒœ í™•ì¸ (Pending ìƒíƒœê°€ 'ì •ìƒ', CNI ì„¤ì¹˜ ì „ì´ê¸° ë•Œë¬¸ì—)
kubectl get pod -n kube-system -l k8s-app=kube-dns -owide

# ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸
ip -c link
ip -c route

# ì—†ëŠ”ê²Œ ì •ìƒ, ë¸Œë¦¿ì§€ êµ¬ì„± í•˜ë‚˜ë„ ì•ˆë˜ì–´ì ¸ ìˆìŒ
brctl show
tree /etc/cni/net.d/
```

#### Flannel CNI ì„¤ì¹˜
```bash
# Namespace ìƒì„±
kubectl create ns kube-flannel
kubectl label --overwrite ns kube-flannel pod-security.kubernetes.io/enforce=privileged

# Helm ë ˆí¬ì§€í† ë¦¬ ì¶”ê°€
helm repo add flannel https://flannel-io.github.io/flannel/
helm repo list
helm search repo flannel

# ì„¤ì • íŒŒì¼ ìƒì„±
cat << EOF > flannel-values.yaml
podCidr: "10.244.0.0/16"

flannel:
  args:
  - "--ip-masq"
  - "--kube-subnet-mgr"
  - "--iface=eth1"  
EOF

# Helm ì„¤ì¹˜
helm install flannel --namespace kube-flannel flannel/flannel -f flannel-values.yaml
helm list -A

# ì„¤ì¹˜ í™•ì¸
kc describe pod -n kube-flannel -l app=flannel
```

#### ì„¤ì¹˜ í›„ ë„¤íŠ¸ì›Œí¬ í™•ì¸
```bash
# CNI ë°”ì´ë„ˆë¦¬ ë° ì„¤ì • í™•ì¸
tree /opt/cni/bin/
tree /etc/cni/net.d/
cat /etc/cni/net.d/10-flannel.conflist | jq

# ConfigMap í™•ì¸
kc describe cm -n kube-flannel kube-flannel-cfg

# ë¼ìš°íŒ… í…Œì´ë¸” í™•ì¸
ip -c route | grep 10.244.

# Worker ë…¸ë“œ ìƒíƒœ í™•ì¸
for i in w1 w2 ; do 
  echo ">> node : k8s-$i <<"
  sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-$i ip -c route
  echo
done

# CoreDNS Running ìƒíƒœ í™•ì¸
kubectl get pod -n kube-system -l k8s-app=kube-dns -owide
```

#### Flannel CNI ë™ì‘
Flannel CNI ëŠ” ê° ë…¸ë“œì— ê³ ìœ í•œ ë„¤íŠ¸ì›Œí¬ ì„œë¸Œë„·ì„ í• ë‹¹í•˜ê³ , ê°™ì€ ë…¸ë“œ ë‚´ì˜ íŒŒë“œê°„ í†µì‹ ì€ `cni0` ë¸Œë¦¿ì§€ë¥¼ í†µí•´ ì²˜ë¦¬í•˜ê³ , <br/>
ë‹¤ë¥¸ ë…¸ë“œì— ìˆëŠ” íŒŒë“œê°„ í†µì‹ í•  ë•ŒëŠ” `flannel.1` í„°ë„ì„ í†µí•´ ì „ë‹¬ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br/>
ë˜í•œ CNI ê°€ ì„¤ì¹˜ë˜ì—ˆê¸° ë•Œë¬¸ì— CoreDNS ê°€ Running ìƒíƒœë¡œ í™œì„±í™” ë©ë‹ˆë‹¤.

### ğŸ“¦ ìƒ˜í”Œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ë° í…ŒìŠ¤íŠ¸

#### ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬
```bash
# ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

# í…ŒìŠ¤íŠ¸ìš© curl íŒŒë“œ ë°°í¬
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
    - name: curl
      image: alpine/curl
      command: ["sleep", "36000"]
EOF
```

#### ë°°í¬ í™•ì¸ ë° í†µì‹  í…ŒìŠ¤íŠ¸
```bash
# ë°°í¬ ìƒíƒœ í™•ì¸
k get all -l app=webpod

# ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ì—ì„œ ë‹¤ë¥¸ ë…¸ë“œì˜ íŒŒë“œì— ì ‘ê·¼
POD1IP=$(k get pods -l app=webpod -owide | awk 'NR==2 {print $6}')
kubectl exec -it curl-pod -- curl $POD1IP

# ì„œë¹„ìŠ¤ IP í™•ì¸ ë° í†µì‹ , DNS ì§ˆì˜ í™•ì¸
k get svc -l app=webpod
kubectl exec -it curl-pod -- nslookup webpod
kubectl exec -it curl-pod -- curl webpod
```

#### iptables ê·œì¹™ í™•ì¸
```bash
# ì„œë¹„ìŠ¤ IP í™•ì¸ ë° iptables ê·œì¹™ ë¶„ì„
SVCIP=$(kubectl get svc webpod -o jsonpath="{.spec.clusterIP}")
iptables -t nat -S | grep $SVCIP

# Worker ë…¸ë“œì˜ iptables ê·œì¹™ í™•ì¸
for i in w1 w2 ; do 
  echo ">> node : k8s-$i <<"
  sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo iptables -t nat -S | grep $SVCIP
  echo
done

# -A KUBE-SERVICES -d 10.96.46.189/32 -p tcp -m comment --comment "default/webpod cluster IP" -m tcp --dport 80 -j KUBE-SVC-CNZCPOCNCNOROALA
# -A KUBE-SVC-CNZCPOCNCNOROALA ! -s 10.244.0.0/16 -d 10.96.46.189/32 -p tcp -m comment --comment "default/webpod cluster IP" -m tcp --dport 80 -j KUBE-MARK-MASQ

# ê·œì¹™ëŒ€ë¡œ ì¡°íšŒí•´ë³´ë©´ ê²°êµ­ 50% í™•ë¥ ë¡œ ê° Pod IP ë¥¼ ê°€ë¦¬í‚¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
iptables -t nat -S | grep KUBE-SVC-CNZCPOCNCNOROALA

# -A KUBE-SVC-CNZCPOCNCNOROALA -m comment --comment "default/webpod -> 10.244.1.2:80" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-PQBQBGZJJ5FKN3TB
# -A KUBE-SVC-CNZCPOCNCNOROALA -m comment --comment "default/webpod -> 10.244.2.2:80" -j KUBE-SEP-WEW7NHLZ4Y5A5ZKF
```

#### ğŸš¨ ëŒ€ê·œëª¨ í™˜ê²½ì—ì„œ iptablesì˜ í•œê³„

ì„œë¹„ìŠ¤ê°€ ì¦ê°€í• ìˆ˜ë¡ iptables ê·œì¹™ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ì„±ëŠ¥ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

### Cilium CNI ì†Œê°œ ë° ì„¤ì¹˜

#### Cilium CNI ì•„í‚¤í…ì²˜

![Cilium Architecture](/assets/cilium/images/ebpf_hostrouting.png)

#### ğŸ› ï¸ Flannel CNI -> Cilium CNI ìµœì†Œ ì¤‘ë‹¨ ë§ˆì´ê·¸ë ˆì´ì…˜

```bash
# Pod CIDR í™•ì¸
kubectl cluster-info dump | grep -m 1 cluster-cidr
```

ì§€ì •ëœ CIDR ì™¸ì— ì¶”ê°€ë¡œ ì„ ì •í•  Pod CIDR ì„¤ì • (ì˜ˆ: `10.245.0.0/16`)

```bash
# Cilium CLI ì„¤ì¹˜
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz >/dev/null 2>&1
tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz

# Helm ì¶”ê°€
helm repo add cilium https://helm.cilium.io/
helm repo update

# Values íŒŒì¼ ì¶”ê°€
cat > cilium-values.yaml <<EOF
operator:
  unmanagedPodWatcher:
    restart: false # Migration: Don't restart unmigrated pods
routingMode: tunnel # Migration: Optional: default is tunneling, configure as needed
tunnelProtocol: vxlan # Migration: Optional: default is VXLAN, configure as needed
tunnelPort: 8473 # Migration: Optional, change only if both networks use the same port by default
cni:
  customConf: true # Migration: Don't install a CNI configuration file
  uninstall: false # Migration: Don't remove CNI configuration on shutdown
ipam:
  mode: "cluster-pool"
  operator:
    clusterPoolIPv4PodCIDRList: ["10.245.0.0/16"] # Migration: Ensure this is distinct and unused
policyEnforcementMode: "never" # Migration: Disable policy enforcement
bpf:
  hostLegacyRouting: true # Migration: Allow for routing between Cilium and the existing overlay
EOF
cilium install --version 1.17.6 --values cilium-values.yaml --dry-run-helm-values > values-initial.yaml

# Cilium ì„¤ì¹˜
helm install cilium cilium/cilium --version 1.17.6 \
  --namespace kube-system \
  --values values-initial.yaml
```

#### Cilium ìƒíƒœ í™•ì¸
```bash
# Cilium Pod ìƒíƒœ í™•ì¸
kubectl -n kube-system get pods -l k8s-app=cilium

# Cilium CNI ìƒíƒœ í™•ì¸
kubectl -n kube-system exec -it $(kubectl -n kube-system get pods -l k8s-app=cilium -o jsonpath='{.items[0].metadata.name}') -- cilium status
```

#### Cilium CNI ì¸ìˆ˜ ì„¤ì •
```bash
# Cilium CNI ì¸ìˆ˜ ì„¤ì •
cat <<EOF | kubectl apply --server-side -f -
apiVersion: cilium.io/v2
kind: CiliumNodeConfig
metadata:
  namespace: kube-system
  name: cilium-default
spec:
  nodeSelector:
    matchLabels:
      io.cilium.migration/cilium-default: "true"
  defaults:
    write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist
    custom-cni-conf: "false"
    cni-chaining-mode: "none"
    cni-exclusive: "true"
EOF
```

#### ë…¸ë“œë³„ ìˆœì°¨ ë§ˆì´ê·¸ë ˆì´ì…˜

ê° ë…¸ë“œì— ëŒ€í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìŒ ì‘ì—… ìˆ˜í–‰:

#### ë…¸ë“œ ì¤€ë¹„
```bash
# ë…¸ë“œ ì´ë¦„ ì„¤ì •
NODE_NAME="k8s-ctr"  # ì‹¤ì œ ë…¸ë“œ ì´ë¦„ìœ¼ë¡œ ë³€ê²½ (ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ë¶€í„° ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)

# ë…¸ë“œ cordon (ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ë°©ì§€)
kubectl cordon $NODE_NAME

# ë…¸ë“œ drain (ê¸°ì¡´ Pod ì´ë™)
kubectl drain $NODE_NAME --ignore-daemonsets --delete-emptydir-data
```

#### ë…¸ë“œ ë¼ë²¨ë§ ë° Cilium ì¬ì‹œì‘
```bash
# Cilium ë§ˆì´ê·¸ë ˆì´ì…˜ ë¼ë²¨ ì¶”ê°€
kubectl label node $NODE_NAME --overwrite "io.cilium.migration/cilium-default=true"

# Cilium ì¬ì‹œì‘
kubectl -n kube-system delete pod --field-selector spec.nodeName=$NODE_NAME -l k8s-app=cilium
kubectl -n kube-system rollout status ds/cilium -w
```

#### ë…¸ë“œ ì¬ë¶€íŒ…
```bash
# í•´ë‹¹ ë…¸ë“œì˜ ë‚´ë¶€ì—ì„œ
sudo reboot
```

#### ë…¸ë“œ ê²€ì¦
```bash
# ë…¸ë“œì˜ Pod ê°€ Cilium CIDR ì— ìˆê³ , API Server ì™€ì˜ ì—°ê²°ì´ ê°€ëŠ¥í•œì§€ ê²€ì¦
kubectl get -o wide node $NODE_NAME
kubectl -n kube-system run --attach --rm --restart=Never verify-network \
  --overrides='{"spec": {"nodeName": "'$NODE_NAME'", "tolerations": [{"operator": "Exists"}]}}' \
  --image ghcr.io/nicolaka/netshoot:v0.8 -- /bin/bash -c 'ip -br addr && curl -s -k https://$KUBERNETES_SERVICE_HOST/healthz && echo'

# NAME     STATUS                     ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
# k8s-w1   Ready,SchedulingDisabled   <none>   11m   v1.33.2   192.168.10.101   <none>        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
# lo               UNKNOWN        127.0.0.1/8 ::1/128 
# eth0@if11        UP             10.245.0.179/32 fe80::ec5c:58ff:fe7e:9837/64 
# ok
# pod "verify-network" deleted

# ë…¸ë“œê°€ Ready ìƒíƒœì¸ì§€ í™•ì¸
kubectl get node $NODE_NAME

# ë…¸ë“œ Pod ìŠ¤ì¼€ì¥´ë§ ë¹„í™œì„±í™” í•´ì œ
kubectl uncordon $NODE_NAME

# Cilium CNI ìƒíƒœ í™•ì¸
kubectl -n kube-system exec -it $(kubectl -n kube-system get pods -l k8s-app=cilium -o jsonpath='{.items[0].metadata.name}') -- cilium status
```

#### ì „ì²´ ë…¸ë“œ ìˆœíšŒ í›„
```bash
# ì „ì²´ Pod ë“¤ì˜ IP ë¶„ë°°ê°€ Cilium ì˜ CIDR ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸
k get pods -A -owide

# ì´ˆê¸° ë§ˆì´ê·¸ë ˆì´ì…˜ ì„¤ì • ê°’ ì™¸ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ ìˆ˜ì •
cilium install \
  --version 1.17.6 \
  --values values-initial.yaml \
  --dry-run-helm-values \
  --set operator.unmanagedPodWatcher.restart=true \
  --set cni.customConf=false \
  --set policyEnforcementMode=default \
  --set bpf.hostLegacyRouting=false \
  --set bpf.masquerade=true \
  --set ipv6.enabled=false \
  --set ipam.mode="cluster-pool" \
  --set ipam.operator.clusterPoolIPv4PodCIDRList={"10.245.0.0/16"} \
  --set ipv4NativeRoutingCIDR=10.245.0.0/16 \
  --set kubeProxyReplacement=true \
  --set k8sServiceHost=192.168.10.100 \
  --set k8sServicePort=6443 \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  > values-final.yaml

# kube-proxy ì œê±°
kubectl -n kube-system delete ds kube-proxy
kubectl -n kube-system delete cm kube-proxy

# Helm Upgrade
helm upgrade --namespace kube-system cilium cilium/cilium --values values-final.yaml
kubectl -n kube-system rollout restart daemonset cilium

# Cilium CNI ìƒíƒœ í™•ì¸
cilium status --wait

# CNI ì¸ìˆ˜ ì„¤ì • í•´ì œ
kubectl delete -n kube-system ciliumnodeconfig cilium-default

```

#### ì´ì „ CNI Flannel ì‚­ì œ
```bash
# Helm ì œê±°
helm uninstall -n kube-flannel flannel
kubectl delete ns kube-flannel

# vnic ì œê±°
ip link del flannel.1
ip link del cni0
for i in w1 w2 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo ip link del flannel.1 ; echo; done
for i in w1 w2 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-$i sudo ip link del cni0 ; echo; done

# IP Table ì´ˆê¸°í™”
iptables-save | grep -v KUBE | grep -v FLANNEL | iptables-restore
iptables-save

sshpass -p 'vagrant' ssh vagrant@k8s-w1 "sudo iptables-save | grep -v KUBE | grep -v FLANNEL | sudo iptables-restore"
sshpass -p 'vagrant' ssh vagrant@k8s-w1 sudo iptables-save

sshpass -p 'vagrant' ssh vagrant@k8s-w2 "sudo iptables-save | grep -v KUBE | grep -v FLANNEL | sudo iptables-restore"
sshpass -p 'vagrant' ssh vagrant@k8s-w2 sudo iptables-save
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

- [Cilium ê³µì‹ ë¬¸ì„œ](https://docs.cilium.io/)
- [eBPF ì†Œê°œ](https://ebpf.io/)
- [Kubernetes ë„¤íŠ¸ì›Œí‚¹ ê°œë…](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
- [Flannel í”„ë¡œì íŠ¸](https://github.com/flannel-io/flannel)

---