---
layout: post
title: "Cilium 네트워킹 핸즈온 가이드: IPAM, Routing, Masquerading, DNS"
date: 2025-07-30 02:30:00 +0900
categories: cilium networking
tags: [cilium, ipam, routing, masquerading, dns, nodelocaldns, kubernetes, ebpf, vagrant]
---

## Cilium 네트워킹 핸즈온 가이드

### 실습 환경 소개
최근 Kubernetes 네트워킹의 복잡성이 증가하면서 Cilium이 제공하는 eBPF 기반의 고급 네트워킹 기능들이 주목받고 있습니다. 이 가이드는 Cilium의 핵심 네트워킹 기능인 IPAM, Routing, Masquerading, DNS를 실습을 통해 깊이 있게 이해할 수 있도록 구성했습니다.

- **클러스터 구성**: Control Plane 1대(k8s-ctr), Worker Node 1대(k8s-w1), Router 1대
- **Router**: 사내망(10.10.0.0/16) 통신 연결, k8s에 join되지 않은 웹 서버
- **Cilium**: v1.17.6

### 실습 환경 확인
```bash
# 클러스터 정보 확인
kubectl cluster-info
kubectl get nodes -owide

# Cilium 상태 확인
cilium status --wait
cilium config view
```

## 1. IPAM (IP Address Management)

### IPAM 소개
IPAM은 네트워크 엔드포인트(컨테이너, 등)에 대한 IP 할당과 관리를 담당합니다. Cilium은 다양한 IPAM 모드를 지원하며, 각 모드는 서로 다른 사용 사례에 최적화되어 있습니다.

### Kubernetes Host Scope IPAM

#### 현재 IPAM 모드 확인
```bash
# IPAM 모드 확인
cilium config view | grep ^ipam

# ipam                                              kubernetes
# ipam-cilium-node-update-rate                      15

# 노드별 PodCIDR 확인
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.podCIDR}{"\n"}{end}'

# k8s-ctr	10.244.0.0/24
# k8s-w1	10.244.1.0/24

# CiliumNode 리소스 확인
kubectl get ciliumnode -o json | grep podCIDRs -A2

#                     "podCIDRs": [
#                         "10.244.0.0/24"
#                     ],
# --
#                     "podCIDRs": [
#                         "10.244.1.0/24"
#                     ],
```

#### 샘플 애플리케이션 배포
```bash
# 웹 애플리케이션 배포 (2개 replica, Anti-Affinity 설정)
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

# curl 테스트용 파드 배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF
```

### Hubble를 통한 네트워크 플로우 모니터링

#### Hubble UI 및 모니터링 설정
```bash
# Hubble UI 접속 정보 확인
NODEIP=$(ip -4 addr show eth1 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
echo -e "Hubble UI: http://$NODEIP:30003"

# Hubble relay 포트 포워딩
cilium hubble port-forward&

# 네트워크 플로우 모니터링
hubble observe -f --pod curl-pod

# 통신 테스트
kubectl exec -it curl-pod -- curl webpod | grep Hostname
```

### Cluster Scope IPAM으로 마이그레이션

#### IPAM 모드 변경 및 확인
```bash
# Cluster Scope로 변경 (새로운 PodCIDR 사용)
helm upgrade cilium cilium/cilium --version 1.17.6 --namespace kube-system --reuse-values \
  --set ipam.mode="cluster-pool" \
  --set ipam.operator.clusterPoolIPv4PodCIDRList={"172.20.0.0/16"} \
  --set ipv4NativeRoutingCIDR=172.20.0.0/16

# 재시작
kubectl -n kube-system rollout restart deploy/cilium-operator
kubectl -n kube-system rollout restart ds/cilium

# 변경 확인
cilium config view | grep ^ipam

# IPAM 모드가 cluster-pool 모드로 변경됨
# ipam                                              cluster-pool
# ipam-cilium-node-update-rate                      15s


kubectl get ciliumnode -o json | grep podCIDRs -A2

#                     "podCIDRs": [
#                         "10.244.0.0/24"
#                     ],
# --
#                     "podCIDRs": [
#                         "10.244.1.0/24"
#                     ],

# 왜 그대로? kubectl get ciliumnode 를 삭제해 주어야 각 노드별 Pod CIDR 이 변경됨
kubectl get ciliumnodes -o name | xargs kubectl delete
kubectl -n kube-system rollout restart deploy/cilium-operator
kubectl -n kube-system rollout restart ds/cilium

kubectl get ciliumnode -o json | grep podCIDRs -A2

#                     "podCIDRs": [
#                         "172.20.1.0/24"
#                     ],
# --
#                     "podCIDRs": [
#                         "172.20.0.0/24"
#                     ],

# 파드 재배포로 새 IP 할당
kubectl rollout restart deploy/webpod
kubectl delete pod curl-pod


# 그럼에도.. 기존 대역대 Pod 들이 남아있음
kubectl get pod -A -owide | grep 10.244.

# 전부다 Pod Delete 후 재생성 하거나.. 합시다.
kubectl get pod -A -o wide | grep '10.244.' | awk '{print $1, $2}' | xargs -n 2 bash -c 'kubectl delete pod -n "$0" "$1"'

# curl 테스트용 파드 재배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# Pod 들 IP 대역 모두 CIDR (172.20.1.0/24) 에 들어오는지 확인
kubectl get pod -A -owide
```

## 2. Routing

### Native Routing 소개
Cilium은 기본적으로 Native Routing을 사용하여 캡슐화 오버헤드를 제거합니다. 이는 네트워크 성능을 크게 향상시킵니다.

### Native Routing 동작 확인

#### 라우팅 테이블 분석
```bash
# 라우팅 모드 확인
cilium config view | grep routing

# 커널 라우팅 테이블 확인
ip -c route | grep 172.20
sshpass -p 'vagrant' ssh vagrant@k8s-w1 ip -c route | grep 172.20

# 노드 간 통신 추적
vagrant ssh k8s-w1
tcpdump -i eth1 icmp -nn
kubectl exec -it curl-pod -- ping <다른_노드의_파드_IP>

# 보내는 곳
# (⎈|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it curl-pod -- ping 172.20.0.156
# PING 172.20.0.156 (172.20.0.156) 56(84) bytes of data.
# 64 bytes from 172.20.0.156: icmp_seq=1 ttl=62 time=0.779 ms
# 64 bytes from 172.20.0.156: icmp_seq=2 ttl=62 time=0.620 m

# 받는 곳
# root@k8s-w1:~# tcpdump -i eth1 icmp -nn
# tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
# listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
# 21:23:23.001630 IP 172.20.1.184 > 172.20.0.156: ICMP echo request, id 7, seq 1, length 64
# 21:23:23.001820 IP 172.20.0.156 > 172.20.1.184: ICMP echo reply, id 7, seq 1, length 64
# 21:23:24.003079 IP 172.20.1.184 > 172.20.0.156: ICMP echo request, id 7, seq 2, length 64
# 21:23:24.003159 IP 172.20.0.156 > 172.20.1.184: ICMP echo reply, id 7, seq 2, length 64
```

### Cilium 디버깅 명령어

#### Cilium Agent 단축키 설정
```bash
# Cilium 파드 단축키 설정
export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-ctr -o jsonpath='{.items[0].metadata.name}')
export CILIUMPOD1=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-w1 -o jsonpath='{.items[0].metadata.name}')

alias c0="kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium"
alias c1="kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- cilium"

# Endpoint 목록 확인
c0 endpoint list
c1 endpoint list

# 실시간 모니터링
c0 monitor -v
```

## 3. Masquerading

### eBPF 기반 Masquerading 소개
Cilium은 eBPF를 사용하여 고성능 Masquerading(NAT)을 제공합니다. 이는 기존 iptables 기반 방식보다 훨씬 효율적입니다.

### Masquerading 동작 확인

#### 현재 설정 확인 및 테스트
```bash
# Masquerading 상태 확인
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium status | grep Masquerading

# Masquerading:            BPF   [eth0, eth1]   172.20.0.0/16 [IPv4: Enabled, IPv6: Disabled]

# Native Routing CIDR 확인
cilium config view | grep ipv4-native-routing-cidr

# Router로 통신 테스트 (NAT 동작 확인)
kubectl exec -it curl-pod -- curl 192.168.10.200

# tcpdump로 소스 IP 확인
tcpdump -i eth1 tcp port 80 -nn

# tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
# listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
# 21:25:18.715025 IP 192.168.10.100.48508 > 192.168.10.200.80: Flags [S], seq 2706058681, win 64240, options [mss 1460,sackOK,TS val 3546382482 ecr 0,nop,wscale 7], length 0
# 21:25:18.715428 IP 192.168.10.200.80 > 192.168.10.100.48508: Flags [S.], seq 2292202534, ack 2706058682, win 65160, options [mss 1460,sackOK,TS val 2384394520 ecr 3546382482,nop,wscale 6], length 0
# 21:25:18.715614 IP 192.168.10.100.48508 > 192.168.10.200.80: Flags [.], ack 1, win 502, options [nop,nop,TS val 3546382483 ecr 2384394520], length 0
# 21:25:18.715855 IP 192.168.10.100.48508 > 192.168.10.200.80: Flags [P.], seq 1:79, ack 1, win 502, options [nop,nop,TS val 3546382483 ecr 2384394520], length 78: HTTP: GET / HTTP/1.1
# 21:25:18.716212 IP 192.168.10.200.80 > 192.168.10.100.48508: Flags [.], ack 79, win 1017, options [nop,nop,TS val 2384394520 ecr 3546382483], length 0
# 21:25:18.716532 IP 192.168.10.200.80 > 192.168.10.100.48508: Flags [P.], seq 1:257, ack 79, win 1017, options [nop,nop,TS val 2384394521 ecr 3546382483], length 256: HTTP: HTTP/1.1 200 OK
# 21:25:18.716569 IP 192.168.10.100.48508 > 192.168.10.200.80: Flags [.], ack 257, win 501, options [nop,nop,TS val 3546382484 ecr 2384394521], length 0
# 21:25:18.717099 IP 192.168.10.100.48508 > 192.168.10.200.80: Flags [F.], seq 79, ack 257, win 501, options [nop,nop,TS val 3546382484 ecr 2384394521], length 0
# 21:25:18.717369 IP 192.168.10.200.80 > 192.168.10.100.48508: Flags [F.], seq 257, ack 80, win 1017, options [nop,nop,TS val 2384394522 ecr 3546382484], length 0
# 21:25:18.717429 IP 192.168.10.100.48508 > 192.168.10.200.80: Flags [.], ack 258, win 501, options [nop,nop,TS val 3546382484 ecr 2384394522], length 0
```

curl 파드에서 통신했지만 소스가 SNAT이 걸려 트래픽 발생 노드 IP로 변경되는 것을 알 수 있다. 이 동작이 마스커레이딩/마스커딩이다.

### ip-masq-agent 설정

#### 특정 대역 NAT 제외 설정
```bash
# ip-masq-agent 활성화 및 사내망 대역 설정
helm upgrade cilium cilium/cilium --version 1.17.6 --namespace kube-system --reuse-values \
  --set ipMasqAgent.enabled=true \
  --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.10.1.0/24,10.10.2.0/24}'

# 설정 확인
kubectl get cm -n kube-system ip-masq-agent -o yaml | yq

# NAT 제외 대역 확인
kubectl -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg bpf ipmasq list

# IP PREFIX/ADDRESS
# 10.10.1.0/24
# 10.10.2.0/24
# 169.254.0.0/16

# Router의 사내망 대역으로 통신 테스트
kubectl exec -it curl-pod -- curl 10.10.1.200

# 21:31:39.785241 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:40.821404 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:41.844923 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:42.868350 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:43.891772 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:44.915315 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:32:41.595784 IP 172.20.1.184.55822 > 10.10.1.200.80: tcp 0
# 21:32:42.614473 IP 172.20.1.184.55822 > 10.10.1.200.80: tcp 0

kubectl exec -it curl-pod -- curl 10.10.2.200

# 21:31:39.785241 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:40.821404 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:41.844923 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:42.868350 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:43.891772 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:31:44.915315 IP 172.20.1.184.44246 > 10.10.1.200.80: tcp 0
# 21:32:41.595784 IP 172.20.1.184.55822 > 10.10.1.200.80: tcp 0
# 21:32:42.614473 IP 172.20.1.184.55822 > 10.10.1.200.80: tcp 0
# 21:32:56.639771 IP 172.20.1.184.53148 > 10.10.2.200.80: tcp 0
# 21:32:57.646954 IP 172.20.1.184.53148 > 10.10.2.200.80: tcp 0

```

nonMasqueradeCIDRs에 목적지 주소가 영역에 걸려서 NAT 없이 Pod IP 그대로 전송되는 것을 알 수 있다.

## 4. CoreDNS와 NodeLocalDNS

### CoreDNS 동작 원리

#### CoreDNS 설정 확인
```bash
# 파드의 DNS 설정 확인
kubectl exec -it curl-pod -- cat /etc/resolv.conf

# /etc/resolv.conf 를 Pod 에 놓아주는 것은 각 노드의 kubelet 서비스이다. (conf 에서 DNS 주소 변경 가능)

# CoreDNS 로깅 활성화
kubectl edit cm -n kube-system coredns
# log와 debug 추가

kubectl get cm -n kube-system coredns -oyaml

# apiVersion: v1
# data:
#   Corefile: |
#     .:53 {
#         log
#         debug
#         errors
#         health {
#            lameduck 5s
#         }
#         ready
#         kubernetes cluster.local in-addr.arpa ip6.arpa {
#            pods insecure
#            fallthrough in-addr.arpa ip6.arpa
#            ttl 30
#         }
#         prometheus :9153
#         forward . /etc/resolv.conf {
#            max_concurrent 1000
#         }
#         cache 30 {
#            disable success cluster.local
#            disable denial cluster.local
#         }
#         loop
#         reload
#         loadbalance
#     }
# kind: ConfigMap
# metadata:
#   creationTimestamp: "2025-08-02T11:17:08Z"
#   name: coredns
#   namespace: kube-system
#   resourceVersion: "10997"
#   uid: 58dcddb5-2dc5-4cfb-8ca7-28d2a97db049

# DNS 질의 모니터링
kubectl -n kube-system logs -l k8s-app=kube-dns -f &
kubectl exec -it curl-pod -- nslookup webpod

# (⎈|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it curl-pod -- nslookup webpod
# [INFO] 172.20.1.184:58304 - 38696 "A IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.00015637s
#                                                           ;; Got recursion not available from 10.96.0.10
# Server:         10.96.0.10
# Address:        10.96.0.10#53

# Name:   webpod.default.svc.cluster.local
# Address: 10.96.193.215
# [INFO] 172.20.1.184:50380 - 11663 "AAAA IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 143 0.000343088s
#                                                                ;; Got recursion not available from 10.96.0.10

kubectl exec -it curl-pod -- nslookup google.com

# (⎈|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it curl-pod -- nslookup google.com
# [INFO] 172.20.1.184:39109 - 20641 "A IN google.com.default.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000294583s
# ;; Got recursion not available from 10.96.0.10
# [INFO] 172.20.1.184:52679 - 3902 "A IN google.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000286602s
#                                                         ;; Got recursion not available from 10.96.0.10
# [INFO] 172.20.1.184:58123 - 37522 "A IN google.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.00025725s
#                                                     ;; Got recursion not available from 10.96.0.10

# [INFO] 172.20.1.184:46117 - 55809 "A IN google.com. udp 28 false 512" NOERROR qr,rd,ra 54 0.123610775s
#                                      Server:            10.96.0.10
# Address:        10.96.0.10#53

# Non-authoritative answer:
# Name:   google.com
# Address: 142.250.66.46
# [INFO] 172.20.1.184:59732 - 10726 "AAAA IN google.com. udp 28 false 512" NOERROR qr,rd,ra 66 0.148730416s
#                                         Name:   google.com
# Address: 2404:6800:4005:813::200e

```

### NodeLocalDNS 설치 및 설정

#### NodeLocalDNS 배포
```bash
# NodeLocalDNS 설정 파일 다운로드
wget https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml

# 변수 설정
kubedns=$(kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP})
domain='cluster.local'
localdns='169.254.20.10'

# sed 적용 (iptables 모드)
sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns.yaml

# 설치
kubectl apply -f nodelocaldns.yaml

# 상태 확인
kubectl get pod -n kube-system -l k8s-app=node-local-dns -owide
```

#### NodeLocalDNS 적용
```bash
# kubelet DNS 서버 설정
sudo sed -i 's/10.96.0.10/169.254.20.10/' /var/lib/kubelet/config.yaml

# 설정적용
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 확인
cat /var/lib/kubelet/config.yaml | grep clusterDNS -A 1

# clusterDNS:
# - 169.254.20.10

# 테스트용 파드 재배포
kubectl delete pod curl-pod

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# 실시간 로그 확인
kubectl -n kube-system logs -l k8s-app=kube-dns -f

# 딱 1번 질의
# [INFO] 10.0.2.15:60708 - 9231 "A IN webpod.default.svc.cluster.local. tcp 50 false 65535" NOERROR qr,aa,rd 98 0.000269125s
# [INFO] 10.0.2.15:60708 - 60242 "AAAA IN webpod.default.svc.cluster.local. tcp 50 false 65535" NOERROR qr,aa,rd 143 0.000063666s

kubectl -n kube-system logs -l k8s-app=node-local-dns -f

# 여러번 질의 -> 하지만 CoreDNS 까지 넘어가지 않음
# [INFO] 172.20.1.65:53828 - 18557 "A IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.001848959s
# [INFO] 172.20.1.65:59925 - 31425 "AAAA IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 143 0.000745625s
# [INFO] 172.20.1.65:41216 - 64163 "A IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000182625s
# [INFO] 172.20.1.65:58860 - 34103 "AAAA IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 143 0.000112167s
# [INFO] 172.20.1.65:33893 - 36673 "A IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000187958s
# [INFO] 172.20.1.65:58817 - 32322 "AAAA IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 143 0.000107417s
# [INFO] 172.20.1.65:60188 - 11751 "A IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 98 0.000121583s
# [INFO] 172.20.1.65:40666 - 29551 "AAAA IN webpod.default.svc.cluster.local. udp 50 false 512" NOERROR qr,aa,rd 143 0.000164584s

kubectl exec -it curl-pod -- cat /etc/resolv.conf
kubectl exec -it curl-pod -- nslookup webpod

```

### Cilium Local Redirect Policy

#### LRP 소개 및 활성화
Local Redirect Policy(LRP)는 특정 트래픽을 노드 로컬 백엔드로 리다이렉트하는 Cilium의 강력한 기능입니다. 이를 통해 네트워크 홉을 줄이고 지연 시간을 최소화할 수 있습니다.

```bash
# Local Redirect Policy 기능 활성화
helm upgrade cilium cilium/cilium --version 1.17.6 --namespace kube-system --reuse-values \
  --set localRedirectPolicy=true

# Cilium 컴포넌트 재시작
kubectl rollout restart deploy cilium-operator -n kube-system
kubectl rollout restart ds cilium -n kube-system

# 활성화 상태 확인
cilium config view | grep -i local-redirect-policy

# enable-local-redirect-policy                      true

```

#### AddressMatcher 테스트를 위한 메타데이터 프록시 애플리케이션 배포
```bash
# 메타데이터 프록시 파드 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metadata-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metadata-proxy
  template:
    metadata:
      labels:
        app: metadata-proxy
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: config
          mountPath: /usr/share/nginx/html
      volumes:
      - name: config
        configMap:
          name: metadata-response
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: metadata-response
data:
  index.html: |
    <html>
    <body>
    <h1>Metadata Service (Local Redirect)</h1>
    <p>Instance ID: i-1234567890abcdef0</p>
    <p>Region: us-east-1</p>
    </body>
    </html>
EOF
```

#### AddressMatcher LRP 정책 생성
```bash
# 169.254.169.254:80으로의 트래픽을 로컬 프록시로 리다이렉트
cat <<EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLocalRedirectPolicy
metadata:
  name: metadata-service-lrp
spec:
  redirectFrontend:
    addressMatcher:
      ip: "169.254.169.254"
      toPorts:
        - port: "80"
          protocol: TCP
  redirectBackend:
    localEndpointSelector:
      matchLabels:
        app: metadata-proxy
    toPorts:
      - port: "80"
        protocol: TCP
EOF

# LRP 생성 확인
kubectl get ciliumlocalredirectpolicies
```

#### 리다이렉트 동작 테스트
```bash
# 테스트 파드에서 메타데이터 서비스 접근
kubectl exec -it curl-pod -- curl -s 169.254.169.254

# Cilium 서비스 엔트리 확인
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg service list | grep 169.254

# LRP 상태 상세 확인
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg lrp list

# LRP namespace   LRP name               FrontendType   Matching Service
# default         metadata-service-lrp   IP + port
#                 |                      169.254.169.254:80/TCP -> 172.20.1.226:80(default/metadata-proxy-8587fc7c46-mfswr),

```

#### ServiceMatcher 테스트를 위한 샘플 서비스 및 로컬 프록시 배포
```bash
# 원본 서비스 배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: echo-service
spec:
  selector:
    app: echo-server
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 80
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: echo-server
  template:
    metadata:
      labels:
        app: echo-server
    spec:
      containers:
      - name: echo
        image: traefik/whoami
        ports:
        - containerPort: 80
EOF

# 로컬 캐시 프록시 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: local-cache-proxy
spec:
  selector:
    matchLabels:
      app: local-cache
  template:
    metadata:
      labels:
        app: local-cache
    spec:
      containers:
      - name: cache
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: cache-config
          mountPath: /etc/nginx/conf.d
      volumes:
      - name: cache-config
        configMap:
          name: nginx-cache-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-cache-config
data:
  default.conf: |
    server {
        listen 80;
        location / {
            add_header X-Cache-Status "LOCAL-HIT";
            return 200 'Response from local cache proxy\n';
        }
    }
EOF
```

#### ServiceMatcher LRP 정책 생성
```bash
# echo-service로의 트래픽을 로컬 캐시로 리다이렉트
cat <<EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLocalRedirectPolicy
metadata:
  name: service-cache-lrp
spec:
  redirectFrontend:
    serviceMatcher:
      serviceName: echo-service
      namespace: default
  redirectBackend:
    localEndpointSelector:
      matchLabels:
        app: local-cache
    toPorts:
      - port: "80"
        protocol: TCP
EOF

# LRP 생성 확인
kubectl get ciliumlocalredirectpolicies
```

#### 리다이렉트 효과 검증
```bash
# 리다이렉트 전후 비교를 위한 모니터링 시작
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium monitor -t drop -t trace -t policy-verdict

# 다른 터미널에서 서비스 접근 테스트
kubectl exec -it curl-pod -- sh -c '
for i in $(seq 1 5); do
  echo "Request $i:"
  curl -s echo-service:8080
  echo ""
done
'

# 응답 헤더 확인 (X-Cache-Status 확인)
kubectl exec -it curl-pod -- curl -I echo-service:8080

# HTTP/1.1 200 OK
# Server: nginx/1.29.0
# Date: Sat, 02 Aug 2025 13:40:20 GMT
# Content-Type: application/octet-stream
# Content-Length: 32
# Connection: keep-alive
# X-Cache-Status: LOCAL-HIT

```

#### NodeLocalDNS용 LRP 설정
```bash
# kubelet DNS 서버 설정 (원복)
sudo sed -i 's/169.254.20.10/10.96.0.10/' /var/lib/kubelet/config.yaml

# 설정적용
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 확인
cat /var/lib/kubelet/config.yaml | grep clusterDNS -A 1

# clusterDNS:
# - 10.96.0.10

# 테스트용 파드 재배포
kubectl delete pod curl-pod

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# CoreDNS 서비스로의 DNS 쿼리를 NodeLocalDNS로 리다이렉트

cat <<EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLocalRedirectPolicy
metadata:
  name: nodelocaldns-lrp
  namespace: kube-system
spec:
  redirectFrontend:
    serviceMatcher:
      serviceName: kube-dns
      namespace: kube-system
      toPorts:
        - port: "53"
          protocol: UDP
        - port: "53"
          protocol: TCP
  redirectBackend:
    localEndpointSelector:
      matchLabels:
        k8s-app: node-local-dns
    toPorts:
      - port: "53"
        protocol: UDP
      - port: "53"
        protocol: TCP
EOF

kubectl get CiliumLocalRedirectPolicy -n kube-system

# DNS 쿼리 리다이렉트 확인
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg lrp list

# LRP namespace   LRP name               FrontendType   Matching Service
# default         metadata-service-lrp   IP + port
#                 |                      169.254.169.254:80/TCP ->
# default         service-cache-lrp      clusterIP + all svc ports   default/echo-service
#                 |                      10.96.171.233:8080/TCP -> 172.20.0.10:80(default/local-cache-proxy-txnw4)

# DNS 성능 테스트
kubectl exec -it curl-pod -- /bin/bash -c '
echo "DNS Query Performance Test:"
time for i in $(seq 1 100); do
  nslookup kubernetes.default.svc.cluster.local >/dev/null 2>&1
done
'

# DNS Query Performance Test:

# real    0m1.272s
# user    0m0.401s
# sys     0m0.541s
```