---
layout: post
title: "Cilium 네트워킹 핸즈온 가이드 (3): BGP Control Plane과 ClusterMesh"
date: 2025-08-16 10:00:00 +0900
categories: cilium kubernetes
tags: [cilium, bgp, clustermesh, kubernetes, ebpf, networking, kind, frr, ecmp]
---

## Cilium 네트워킹 핸즈온 가이드 (3)

엔터프라이즈 환경에서 Kubernetes 클러스터를 운영하다 보면 BGP를 통한 네트워크 통합과 멀티 클러스터 간 연결이 필수적입니다. 이번 포스트에서는 Cilium의 BGP Control Plane과 ClusterMesh를 활용한 고급 네트워킹 기능을 실습을 통해 알아보겠습니다.

### 실습 환경 소개

이번 실습에서는 BGP Control Plane 테스트를 위해 실제 네트워크 장비를 시뮬레이션하는 환경을 구성합니다.

- Control Plane(k8s-ctr): 192.168.10.100
- Worker Node 1(k8s-w1): 192.168.10.101
- Worker Node 2(k8s-w0): 192.168.20.100 (다른 서브넷)
- Router(FRR): 192.168.10.200, 192.168.20.200

### 기본 환경 확인
```bash
# k8s-ctr 노드 접속
vagrant ssh k8s-ctr

# 클러스터 정보 확인
kubectl cluster-info
kubectl get node -owide

# Cilium 설치 상태 확인
cilium status
cilium config view | grep -i bgp

# BGP Control Plane 활성화 확인
# bgp-router-id-allocation-ip-pool                  
# bgp-router-id-allocation-mode                     default
# bgp-secrets-namespace                             kube-system
# enable-bgp-control-plane                          true
# enable-bgp-control-plane-status-report            true
```

---

## Cilium BGP Control Plane

### BGP Control Plane 소개

Cilium BGP Control Plane (BGPv2)은 Cilium Custom Resources를 통해 BGP 설정을 관리할 수 있는 강력한 기능입니다. 기존 네트워크 인프라와의 통합을 위해 표준 BGP 프로토콜을 지원합니다.

### 샘플 애플리케이션 배포
```bash
# 웹 애플리케이션 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

# 테스트용 curl 파드 배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail", "-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# 배포 확인
kubectl get pod -owide
kubectl get svc webpod
```

### 초기 통신 문제 확인

```bash
# 통신 테스트 (타임아웃 발생)
# 다른 서브넷간 통신 불가
kubectl exec -it curl-pod -- curl -s --connect-timeout 1 webpod

# 같은 서브넷의 webpod 는 통신됨
# Hostname: webpod-65794679cc-7dbj2
# IP: 127.0.0.1
# IP: ::1
# IP: 172.20.0.11
# IP: fe80::c45:d1ff:fe3f:f4cf
# RemoteAddr: 172.20.0.4:38988
# GET / HTTP/1.1
# Host: webpod
# User-Agent: curl/8.14.1
# Accept: */*

# 다른 서브넷의 webpod 는 통신안됨
# command terminated with exit code 28
```

### Router FRR 설정

#### FRR BGP 구성

FRR(Free Range Routing)은 Linux 플랫폼에서 실행되는 무료 오픈 소스 라우팅 소프트웨어입니다. FRR은 BGP, OSPF, IS-IS, RIP, PIM, LDP 등 다양한 라우팅 프로토콜을 지원합니다.

- **멀티 프로토콜 지원**: BGP, OSPF, IS-IS, RIP, PIM, LDP 등 주요 라우팅 프로토콜 지원
- **모듈식 아키텍처**: 각 프로토콜이 독립적인 데몬으로 실행되어 안정성 향상
- **표준 준수**: RFC 표준을 준수하여 다른 벤더 장비와 호환성 보장
- **확장성**: 대규모 네트워크 환경에서도 안정적인 성능 제공

본 실습에서는 FRR을 사용하여 Router에서 BGP를 구성하고, Cilium 노드들과 BGP 피어링을 설정합니다. 이를 통해 Pod CIDR 정보를 라우팅 테이블에 전파하여 클러스터 간 통신을 가능하게 합니다.

```bash
# Router 접속 (새 터미널)
vagrant ssh router

# FRR 상태 확인
ps -ef | grep frr
sudo vtysh -c 'show running'
sudo vtysh -c 'show ip bgp summary'

# BGP가 활성화되어 있으나 아직 피어는 없는 상태
# Building configuration...
# Current configuration:
# !
# frr version 8.4.4
# frr defaults traditional
# hostname router
# log syslog informational
# no ipv6 forwarding
# service integrated-vtysh-config
# !
# router bgp 65000
#  bgp router-id 192.168.10.200
#  no bgp ebgp-requires-policy
#  bgp graceful-restart
#  bgp bestpath as-path multipath-relax
#  !
#  address-family ipv4 unicast
#   network 10.10.1.0/24
#   maximum-paths 4
#  exit-address-family
# exit
# !
# end
# % No BGP neighbors found in VRF default

# FRR에 Cilium 노드를 BGP 이웃으로 추가
sudo vtysh
conf
router bgp 65000
  neighbor CILIUM peer-group
  neighbor CILIUM remote-as external
  neighbor 192.168.10.100 peer-group CILIUM
  neighbor 192.168.10.101 peer-group CILIUM
  neighbor 192.168.20.100 peer-group CILIUM
end
write memory
exit

# FRR BGP 피어 확인
sudo vtysh -c 'show ip bgp summary'

# 3개 피어 등록
# IPv4 Unicast Summary (VRF default):
# BGP router identifier 192.168.10.200, local AS number 65000 vrf-id 0
# BGP table version 1
# RIB entries 1, using 192 bytes of memory
# Peers 3, using 2172 KiB of memory
# Peer groups 1, using 64 bytes of memory

# Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc
# 192.168.10.100  4          0         0         0        0    0    0    never       Active        0 N/A
# 192.168.10.101  4          0         0         0        0    0    0    never       Active        0 N/A
# 192.168.20.100  4          0         0         0        0    0    0    never       Active        0 N/A

# Total number of neighbors 3

# FRR 재시작
sudo systemctl restart frr

# 모니터링 (별도 터미널에서 실행)
sudo journalctl -u frr -f


# Aug 16 19:03:03 router watchfrr[4620]: [YFT0P-5Q5YX] Forked background command [pid 4621]: /usr/lib/frr/watchfrr.sh restart all
# Aug 16 19:03:03 router zebra[4633]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
# Aug 16 19:03:03 router bgpd[4638]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
# Aug 16 19:03:03 router staticd[4645]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
# Aug 16 19:03:03 router watchfrr[4620]: [QDG3Y-BY5TN] zebra state -> up : connect succeeded
# Aug 16 19:03:03 router watchfrr[4620]: [QDG3Y-BY5TN] bgpd state -> up : connect succeeded
# Aug 16 19:03:03 router watchfrr[4620]: [QDG3Y-BY5TN] staticd state -> up : connect succeeded
# Aug 16 19:03:03 router watchfrr[4620]: [KWE5Q-QNGFC] all daemons up, doing startup-complete notify
# Aug 16 19:03:03 router frrinit.sh[4610]:  * Started watchfrr
# Aug 16 19:03:03 router systemd[1]: Started frr.service - FRRouting.
```

### Cilium BGP 설정

#### 노드 라벨링 및 BGP Advertisement 구성

BGP를 사용할 노드를 라벨링하고 BGP 리소스를 생성합니다.

```bash
# BGP를 사용할 노드 라벨링
kubectl label nodes k8s-ctr k8s-w0 k8s-w1 enable-bgp=true
kubectl get node -l enable-bgp=true

# BGP 설정 적용
cat <<EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "PodCIDR"
---
apiVersion: cilium.io/v2
kind: CiliumBGPPeerConfig
metadata:
  name: cilium-peer
spec:
  timers:
    holdTimeSeconds: 9
    keepAliveTimeSeconds: 3
  ebgpMultihop: 2
  gracefulRestart:
    enabled: true
    restartTimeSeconds: 15
  families:
    - afi: ipv4
      safi: unicast
      advertisements:
        matchLabels:
          advertise: "bgp"
---
apiVersion: cilium.io/v2
kind: CiliumBGPClusterConfig
metadata:
  name: cilium-bgp
spec:
  nodeSelector:
    matchLabels:
      "enable-bgp": "true"
  bgpInstances:
  - name: "instance-65001"
    localASN: 65001
    peers:
    - name: "tor-switch"
      peerASN: 65000
      peerAddress: 192.168.10.200
      peerConfigRef:
        name: "cilium-peer"
EOF

# router 노드의 로그에서 bgp_update 시그널 확인 가능
# sudo journalctl -u frr -f
# Aug 16 19:09:26 router bgpd[4638]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.20.100 in vrf default
# Aug 16 19:09:26 router bgpd[4638]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.101 in vrf default
# Aug 16 19:09:26 router bgpd[4638]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.100 in vrf default

```

### BGP 연결 확인

```bash
# BGP 피어 상태 확인
cilium bgp peers

# Node      Local AS   Peer AS   Peer Address     Session State   Uptime   Family         Received   Advertised
# k8s-ctr   65001      65000     192.168.10.200   established     38s      ipv4/unicast   4          2    
# k8s-w0    65001      65000     192.168.10.200   established     39s      ipv4/unicast   4          2    
# k8s-w1    65001      65000     192.168.10.200   established     39s      ipv4/unicast   4          2 

# BGP 라우트 확인
cilium bgp routes available ipv4 unicast

# Node      VRouter   Prefix          NextHop   Age    Attrs
# k8s-ctr   65001     172.20.0.0/24   0.0.0.0   1m7s   [{Origin: i} {Nexthop: 0.0.0.0}]   
# k8s-w0    65001     172.20.2.0/24   0.0.0.0   1m7s   [{Origin: i} {Nexthop: 0.0.0.0}]   
# k8s-w1    65001     172.20.1.0/24   0.0.0.0   1m7s   [{Origin: i} {Nexthop: 0.0.0.0}]

# 이 과정을 통해 Cilium이 BGP를 통해 외부 라우터와 정상적으로 통신하고, Kubernetes Pod 네트워크 대역이 외부 네트워크로 광고되고 있는지 확인할 수 있습니다.

# CiliumBGP 리소스 확인
kubectl get ciliumbgpadvertisements,ciliumbgppeerconfigs,ciliumbgpclusterconfigs

# Router에서 BGP 라우팅 테이블 확인
vagrant ssh router
sudo vtysh -c 'show ip bgp summary'
sudo vtysh -c 'show ip bgp'
ip route | grep bgp

# 172.20.0.0/24 nhid 32 via 192.168.10.100 dev eth1 proto bgp metric 20 
# 172.20.1.0/24 nhid 28 via 192.168.10.101 dev eth1 proto bgp metric 20 
# 172.20.2.0/24 nhid 31 via 192.168.20.100 dev eth2 proto bgp metric 20
```

### 통신 문제 해결

#### Pod 간 통신을 위한 라우팅 설정

BGP로 PodCIDR 정보는 교환되지만, Cilium은 기본적으로 외부 경로를 커널 라우팅 테이블에 주입하지 않습니다. <br/>
따라서 이 경우 수동으로 라우팅 테이블을 추가해야 합니다.

```bash
# k8s 파드 대역(172.20.0.0/16)에 대한 라우팅 추가
sudo ip route add 172.20.0.0/16 via 192.168.10.200
sshpass -p vagrant ssh vagrant@k8s-w1 "sudo ip route add 172.20.0.0/16 via 192.168.10.200"
sshpass -p vagrant ssh vagrant@k8s-w0 "sudo ip route add 172.20.0.0/16 via 192.168.20.200"

# 각 노드에서 라우팅 테이블(ip route) 확인
ip route | grep 172.20
sshpass -p vagrant ssh vagrant@k8s-w1 "ip route | grep 172.20"
sshpass -p vagrant ssh vagrant@k8s-w0 "ip route | grep 172.20"

# 통신 테스트 (성공)
kubectl exec -it curl-pod -- curl -s webpod | grep Hostname

# Hubble로 플로우 모니터링
cilium hubble port-forward&
hubble observe -f --protocol tcp --pod curl-pod

# Aug 16 15:00:53.736: default/curl-pod:38124 (ID:7810) <- default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK, PSH)
# Aug 16 15:00:53.737: default/curl-pod:38124 (ID:7810) -> default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK, FIN)
# Aug 16 15:00:53.737: default/curl-pod:38124 (ID:7810) <- default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK, FIN)
# Aug 16 15:00:53.737: default/curl-pod:38124 (ID:7810) -> default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK)
```

---

## Service LoadBalancer IP BGP 광고

### LoadBalancer IP Pool 생성

BGP를 통해 Service LoadBalancer IP를 외부에 광고하기 위해 먼저 IP Pool을 생성합니다.

```bash
# IP Pool 정의
cat << EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "cilium-pool"
spec:
  allowFirstLastIPs: "No"
  blocks:
  - cidr: "172.16.1.0/24"
EOF

# Pool 상태 확인
kubectl get ippool

# Service를 LoadBalancer로 변경
kubectl patch svc webpod -p '{"spec": {"type": "LoadBalancer"}}'

# External IP 확인
kubectl get svc webpod
LBIP=$(kubectl get svc webpod -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "LoadBalancer IP: $LBIP"
```

### LoadBalancer IP BGP 광고 설정

LoadBalancer IP를 BGP 프로토콜로 광고하도록 설정합니다.

```bash
cat << EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements-lb-exip-webpod
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "Service"
      service:
        addresses:
          - LoadBalancerIP
      selector:             
        matchExpressions:
          - { key: app, operator: In, values: [ webpod ] }
EOF

# BGP 라우트 정책 확인
kubectl exec -it -n kube-system ds/cilium -- cilium-dbg bgp route-policies

# VRouter   Policy Name                                             Type     Match Peers         Match Families   Match Prefixes (Min..Max Len)   RIB Action   Path Actions
# 65001     allow-local                                             import                                                                        accept       
# 65001     tor-switch-ipv4-PodCIDR                                 export   192.168.10.200/32                    172.20.1.0/24 (24..24)          accept       
# 65001     tor-switch-ipv4-Service-webpod-default-LoadBalancerIP   export   192.168.10.200/32                    172.16.1.1/32 (32..32)          accept 

# Router에서 BGP 라우트 확인
sshpass -p 'vagrant' ssh vagrant@router ip -c route | grep bgp

# VIP 생성 확인
# 172.16.1.1 nhid 40 proto bgp metric 20 
# 172.20.0.0/24 nhid 32 via 192.168.10.100 dev eth1 proto bgp metric 20 
# 172.20.1.0/24 nhid 28 via 192.168.10.101 dev eth1 proto bgp metric 20 
# 172.20.2.0/24 nhid 31 via 192.168.20.100 dev eth2 proto bgp metric 20 

sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp 172.16.1.1/32'"

# VIP 에 대한 BGP Routing Table 생성 확인
# BGP routing table entry for 172.16.1.1/32, version 11
# Paths: (3 available, best #2, table default)
#   Advertised to non peer-group peers:
#   192.168.10.100 192.168.10.101 192.168.20.100
#   65001
#     192.168.20.100 from 192.168.20.100 (192.168.20.100)
#       Origin IGP, valid, external, multipath
#       Last update: Sat Aug 16 19:29:12 2025
#   65001
#     192.168.10.100 from 192.168.10.100 (192.168.10.100)
#       Origin IGP, valid, external, multipath, best (Router ID)
#       Last update: Sat Aug 16 19:29:12 2025
#   65001
#     192.168.10.101 from 192.168.10.101 (192.168.10.101)
#       Origin IGP, valid, external, multipath
#       Last update: Sat Aug 16 19:29:12 2025

# Router에서 LoadBalancer IP로 접근 (정상 확인)
sshpass -p 'vagrant' ssh vagrant@router curl -s $LBIP | grep Hostname
```

### External Traffic Policy

#### Local 모드 설정 (소스 IP 보존)

External Traffic Policy를 Local로 설정하면 실제 파드가 있는 노드만 BGP로 광고됩니다.

```bash
# External Traffic Policy를 Local로 변경
kubectl patch service webpod -p '{"spec":{"externalTrafficPolicy":"Local"}}'

# BGP 라우트 확인 (파드가 있는 노드만 광고)
sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp'"
sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp 172.16.1.1/32'"
sshpass -p 'vagrant' ssh vagrant@router "ip -c route"

# 접속 테스트
LBIP=172.16.1.1
for i in {1..100}; do
  curl -s $LBIP | grep Hostname
done | sort | uniq -c | sort -nr
```

#### ECMP Hash Policy 최적화

ECMP(Equal Cost Multi-Path)로 로드밸런싱 효율을 높이기 위해 Hash Policy를 최적화합니다.

```bash
# Linux ECMP Hash Policy를 L4 기반으로 변경
sshpass -p 'vagrant' ssh vagrant@router << 'EOF'
sudo sysctl -w net.ipv4.fib_multipath_hash_policy=1
sudo echo "net.ipv4.fib_multipath_hash_policy=1" >> /etc/sysctl.conf
EOF

# 부하분산 테스트
for i in {1..100}; do
  curl -s $LBIP | grep Hostname
done | sort | uniq -c | sort -nr
```

---

## ClusterMesh로 멀티 클러스터 구성

### Kind 클러스터 배포

ClusterMesh 실습을 위해 Kind(Kubernetes in Docker)로 두 개의 클러스터를 생성합니다.

#### Kind 설치

Kind(Kubernetes in Docker)가 설치되어 있지 않다면 다음 명령어로 설치합니다.

```bash
brew install kind
```

#### West/East 클러스터 생성

```bash
# West 클러스터 생성
kind create cluster --name west --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
- role: worker
networking:
  podSubnet: "10.0.0.0/16"
  serviceSubnet: "10.2.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# East 클러스터 생성
kind create cluster --name east --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 31000
    hostPort: 31000
  - containerPort: 31001
    hostPort: 31001
- role: worker
networking:
  podSubnet: "10.1.0.0/16"
  serviceSubnet: "10.3.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# 컨텍스트 확인
kubectl config get-contexts

# alias 설정
alias kwest='kubectl --context kind-west'
alias keast='kubectl --context kind-east'

# 노드 확인
kwest get node -owide
keast get node -owide

# NAME                 STATUS     ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION                         CONTAINER-RUNTIME
# west-control-plane   NotReady   control-plane   3m25s   v1.33.2   192.168.97.3   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
# west-worker          NotReady   <none>          3m12s   v1.33.2   192.168.97.2   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
# NAME                 STATUS     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION                         CONTAINER-RUNTIME
# east-control-plane   NotReady   control-plane   62s   v1.33.2   192.168.97.4   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
# east-worker          NotReady   <none>          48s   v1.33.2   192.168.97.5   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
```

### Cilium CNI 설치

ClusterMesh를 위한 Cilium을 각 클러스터에 설치합니다.

```bash
# Cilium CNI 설치
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "arm64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-darwin-${CLI_ARCH}.tar.gz{,.sha256sum}
shasum -a 256 -c cilium-darwin-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-darwin-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-darwin-${CLI_ARCH}.tar.gz{,.sha256sum}


> **⚠️ 주의:** 아래 내용은 1.18.0 테스트시 글로벌 서비스 클러스터 메시 분산처리가 정상적으로 이루어지지 않았습니다. 테스트를 해보신 분들은 진행 하신 후 기존 컨텍스트의 Cilium 을 날리고 1.17.6 버전으로 다시 테스트 하시길 추천드립니다.

# Cilium 기존 배포 삭제
cilium uninstall --context kind-west
cilium uninstall --context kind-east

# West 클러스터 Cilium 설치
cilium install --version 1.17.6 \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set bpf.masquerade=true \
  --set endpointHealthChecking.enabled=false \
  --set healthChecking=false \
  --set operator.replicas=1 \
  --set debug.enabled=true \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  --set ipv4NativeRoutingCIDR=10.0.0.0/16 \
  --set ipMasqAgent.enabled=true \
  --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.1.0.0/16}' \
  --set cluster.name=west \
  --set cluster.id=1 \
  --context kind-west

# East 클러스터 Cilium 설치
cilium install --version 1.17.6 \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set bpf.masquerade=true \
  --set endpointHealthChecking.enabled=false \
  --set healthChecking=false \
  --set operator.replicas=1 \
  --set debug.enabled=true \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  --set ipv4NativeRoutingCIDR=10.1.0.0/16 \
  --set ipMasqAgent.enabled=true \
  --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.0.0.0/16}' \
  --set cluster.name=east \
  --set cluster.id=2 \
  --context kind-east

# 상태 확인
cilium status --context kind-west
cilium status --context kind-east
```

### ClusterMesh 설정

#### 클러스터 연결

두 클러스터를 ClusterMesh로 연결합니다.

```bash
# Shared Certificate Authority 설정
keast delete secret -n kube-system cilium-ca
kubectl --context kind-west get secret -n kube-system cilium-ca -o yaml | \
kubectl --context kind-east create -f -

# ClusterMesh 활성화
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-west
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-east

# 클러스터 연결
cilium clustermesh connect --context kind-west --destination-context kind-east

# 상태 확인
cilium clustermesh status --context kind-west --wait
cilium clustermesh status --context kind-east --wait

# 노드 목록 확인 (양쪽 클러스터 노드가 모두 보임)
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium node list
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium node list

# Name                      IPv4 Address   Endpoint CIDR   IPv6 Address   Endpoint CIDR   Source
# east/east-control-plane   192.168.97.4   10.1.0.0/24                                    clustermesh
# east/east-worker          192.168.97.5   10.1.1.0/24                                    clustermesh
# west/west-control-plane   192.168.97.3   10.0.0.0/24                                    local
# west/west-worker          192.168.97.2   10.0.1.0/24                                    custom-resource
# Name                      IPv4 Address   Endpoint CIDR   IPv6 Address   Endpoint CIDR   Source
# east/east-control-plane   192.168.97.4   10.1.0.0/24                                    custom-resource
# east/east-worker          192.168.97.5   10.1.1.0/24                                    local
# west/west-control-plane   192.168.97.3   10.0.0.0/24                                    clustermesh
# west/west-worker          192.168.97.2   10.0.1.0/24                                    clustermesh
```

### Global Service 배포

멀티 클러스터 환경에서 Global Service를 배포하여 클러스터 간 서비스 디스커버리를 구현합니다.

```bash
# 각 컨트롤 플레인 노드에 대하여 Pod 배치될 수 있도록 Taint 설정 해제
keast taint nodes east-control-plane node-role.kubernetes.io/control-plane:NoSchedule-
kwest taint nodes west-control-plane node-role.kubernetes.io/control-plane:NoSchedule-

# West 클러스터에 서비스 배포
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  annotations:
    service.cilium.io/global: "true"
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

# East 클러스터에도 동일 서비스 배포
cat << EOF | kubectl apply --context kind-east -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  annotations:
    service.cilium.io/global: "true"
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

kwest get svc -l app=webpod
kwest get pods -l app=webpod -owide
keast get svc -l app=webpod
keast get pods -l app=webpod -owide

# NAME     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
# webpod   ClusterIP   10.2.175.143   <none>        80/TCP    56s
# NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
# webpod-74f6f7bd86-5d54n   1/1     Running   0          56s   10.0.1.75    west-worker          <none>           <none>
# webpod-74f6f7bd86-tfrmk   1/1     Running   0          56s   10.0.0.103   west-control-plane   <none>           <none>

# NAME     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
# webpod   ClusterIP   10.3.128.57   <none>        80/TCP    50s
# NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
# webpod-74f6f7bd86-f5sq9   1/1     Running   0          50s   10.1.0.141   east-control-plane   <none>           <none>
# webpod-74f6f7bd86-zzg4b   1/1     Running   0          50s   10.1.1.36    east-worker          <none>           <none>

# 서비스 엔드포인트 확인
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 14   10.2.175.143:80/TCP      ClusterIP      1 => 10.0.1.75:80/TCP (active)        
#                                              2 => 10.0.0.103:80/TCP (active)       
#                                              3 => 10.1.0.141:80/TCP (active)       
#                                              4 => 10.1.1.36:80/TCP (active) 

keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 14   10.3.128.57:80/TCP       ClusterIP      1 => 10.0.0.103:80/TCP (active)       
#                                              2 => 10.0.1.75:80/TCP (active)        
#                                              3 => 10.1.0.141:80/TCP (active)       
#                                              4 => 10.1.1.36:80/TCP (active)

```

#### 테스트용 Pod 배포

각 클러스터에 테스트용 Pod를 배포하여 크로스 클러스터 통신을 확인합니다.

```bash
# West 클러스터에 curl-pod 배포
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# East 클러스터에 curl-pod 배포
cat << EOF | kubectl apply --context kind-east -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# 통신 테스트 (양쪽 클러스터의 Pod에서 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

kubectl exec -it curl-pod --context kind-east -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# Local affinity 설정
kwest annotate service webpod service.cilium.io/affinity=local --overwrite
keast annotate service webpod service.cilium.io/affinity=local --overwrite

# 상태 확인
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 자기 위치의 노드에 있는 EndPoint preferred 마킹 추가
# 14   10.2.175.143:80/TCP      ClusterIP      1 => 10.0.1.75:80/TCP (active) (preferred)    
#                                              2 => 10.0.0.103:80/TCP (active) (preferred)   
#                                              3 => 10.1.0.141:80/TCP (active)               
#                                              4 => 10.1.1.36:80/TCP (active) 

keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 자기 위치의 노드에 있는 EndPoint preferred 마킹 추가
# 14   10.3.128.57:80/TCP       ClusterIP      1 => 10.0.0.103:80/TCP (active)               
#                                              2 => 10.0.1.75:80/TCP (active)                
#                                              3 => 10.1.0.141:80/TCP (active) (preferred)   
#                                              4 => 10.1.1.36:80/TCP (active) (preferred)

# 테스트 (로컬 클러스터의 Pod에서만 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# 자기 위치의 노드에 EndPoint 가 없는 상태 테스트
kwest scale deployment webpod --replicas 0

# 테스트 (다른 클러스터의 Pod에서만 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# 원복
kwest scale deployment webpod --replicas 2

# Remote affinity 설정
kwest annotate service webpod service.cilium.io/affinity=remote --overwrite
keast annotate service webpod service.cilium.io/affinity=remote --overwrite

# 테스트 (원격 클러스터의 Pod에서 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'
```

---
