---
layout: post
title: "Cilium 네트워킹 핸즈온 가이드 (3): BGP Control Plane과 ClusterMesh"
date: 2025-08-16 10:00:00 +0900
categories: cilium kubernetes
tags: [cilium, bgp, clustermesh, kubernetes, ebpf, networking, kind, frr, ecmp]
---

## Cilium 네트워킹 핸즈온 가이드 (3)

엔터프라이즈 환경에서 Kubernetes 클러스터를 운영하다 보면 BGP를 통한 네트워크 통합과 멀티 클러스터 간 연결이 필수적입니다. 이번 포스트에서는 Cilium의 BGP Control Plane과 ClusterMesh를 활용한 고급 네트워킹 기능을 실습을 통해 알아보겠습니다.

### 실습 환경 소개

이번 실습에서는 BGP Control Plane 테스트를 위해 실제 네트워크 장비를 시뮬레이션하는 환경을 구성합니다.

- Control Plane(k8s-ctr): 192.168.10.100
- Worker Node 1(k8s-w1): 192.168.10.101
- Worker Node 2(k8s-w0): 192.168.20.100 (다른 서브넷)
- Router(FRR): 192.168.10.200, 192.168.20.200

### 기본 환경 확인
```bash
# k8s-ctr 노드 접속
vagrant ssh k8s-ctr

# 클러스터 정보 확인
kubectl cluster-info
kubectl get node -owide

# Cilium 설치 상태 확인
cilium status
cilium config view | grep -i bgp

# BGP Control Plane 활성화 확인
# bgp-router-id-allocation-ip-pool                  
# bgp-router-id-allocation-mode                     default
# bgp-secrets-namespace                             kube-system
# enable-bgp-control-plane                          true
# enable-bgp-control-plane-status-report            true
```

---

## Cilium BGP Control Plane

### BGP Control Plane 소개

Cilium BGP Control Plane (BGPv2)은 Cilium Custom Resources를 통해 BGP 설정을 관리할 수 있는 강력한 기능입니다. 기존 네트워크 인프라와의 통합을 위해 표준 BGP 프로토콜을 지원합니다.

### 샘플 애플리케이션 배포
```bash
# 웹 애플리케이션 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

# 테스트용 curl 파드 배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail", "-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# 배포 확인
kubectl get pod -owide
kubectl get svc webpod
```

### 초기 통신 문제 확인

```bash
# 통신 테스트 (타임아웃 발생)
# 다른 서브넷간 통신 불가
kubectl exec -it curl-pod -- curl -s --connect-timeout 1 webpod

# 같은 서브넷의 webpod 는 통신됨
# Hostname: webpod-65794679cc-7dbj2
# IP: 127.0.0.1
# IP: ::1
# IP: 172.20.0.11
# IP: fe80::c45:d1ff:fe3f:f4cf
# RemoteAddr: 172.20.0.4:38988
# GET / HTTP/1.1
# Host: webpod
# User-Agent: curl/8.14.1
# Accept: */*

# 다른 서브넷의 webpod 는 통신안됨
# command terminated with exit code 28
```

### Router FRR 설정

#### FRR BGP 구성

FRR(Free Range Routing)은 Linux 플랫폼에서 실행되는 무료 오픈 소스 라우팅 소프트웨어입니다. FRR은 BGP, OSPF, IS-IS, RIP, PIM, LDP 등 다양한 라우팅 프로토콜을 지원합니다.

- **멀티 프로토콜 지원**: BGP, OSPF, IS-IS, RIP, PIM, LDP 등 주요 라우팅 프로토콜 지원
- **모듈식 아키텍처**: 각 프로토콜이 독립적인 데몬으로 실행되어 안정성 향상
- **표준 준수**: RFC 표준을 준수하여 다른 벤더 장비와 호환성 보장
- **확장성**: 대규모 네트워크 환경에서도 안정적인 성능 제공

본 실습에서는 FRR을 사용하여 Router에서 BGP를 구성하고, Cilium 노드들과 BGP 피어링을 설정합니다. 이를 통해 Pod CIDR 정보를 라우팅 테이블에 전파하여 클러스터 간 통신을 가능하게 합니다.

```bash
# Router 접속 (새 터미널)
vagrant ssh router

# FRR 상태 확인
ps -ef | grep frr
sudo vtysh -c 'show running'
sudo vtysh -c 'show ip bgp summary'

# BGP가 활성화되어 있으나 아직 피어는 없는 상태
# Building configuration...
# Current configuration:
# !
# frr version 8.4.4
# frr defaults traditional
# hostname router
# log syslog informational
# no ipv6 forwarding
# service integrated-vtysh-config
# !
# router bgp 65000
#  bgp router-id 192.168.10.200
#  no bgp ebgp-requires-policy
#  bgp graceful-restart
#  bgp bestpath as-path multipath-relax
#  !
#  address-family ipv4 unicast
#   network 10.10.1.0/24
#   maximum-paths 4
#  exit-address-family
# exit
# !
# end
# % No BGP neighbors found in VRF default

# FRR에 Cilium 노드를 BGP 이웃으로 추가
sudo vtysh
conf
router bgp 65000
  neighbor CILIUM peer-group
  neighbor CILIUM remote-as external
  neighbor 192.168.10.100 peer-group CILIUM
  neighbor 192.168.10.101 peer-group CILIUM
  neighbor 192.168.20.100 peer-group CILIUM
end
write memory
exit

# FRR BGP 피어 확인
sudo vtysh -c 'show ip bgp summary'

# 3개 피어 등록
# IPv4 Unicast Summary (VRF default):
# BGP router identifier 192.168.10.200, local AS number 65000 vrf-id 0
# BGP table version 1
# RIB entries 1, using 192 bytes of memory
# Peers 3, using 2172 KiB of memory
# Peer groups 1, using 64 bytes of memory

# Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc
# 192.168.10.100  4          0         0         0        0    0    0    never       Active        0 N/A
# 192.168.10.101  4          0         0         0        0    0    0    never       Active        0 N/A
# 192.168.20.100  4          0         0         0        0    0    0    never       Active        0 N/A

# Total number of neighbors 3

# FRR 재시작
sudo systemctl restart frr

# 모니터링 (별도 터미널에서 실행)
sudo journalctl -u frr -f


# Aug 16 19:03:03 router watchfrr[4620]: [YFT0P-5Q5YX] Forked background command [pid 4621]: /usr/lib/frr/watchfrr.sh restart all
# Aug 16 19:03:03 router zebra[4633]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
# Aug 16 19:03:03 router bgpd[4638]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
# Aug 16 19:03:03 router staticd[4645]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
# Aug 16 19:03:03 router watchfrr[4620]: [QDG3Y-BY5TN] zebra state -> up : connect succeeded
# Aug 16 19:03:03 router watchfrr[4620]: [QDG3Y-BY5TN] bgpd state -> up : connect succeeded
# Aug 16 19:03:03 router watchfrr[4620]: [QDG3Y-BY5TN] staticd state -> up : connect succeeded
# Aug 16 19:03:03 router watchfrr[4620]: [KWE5Q-QNGFC] all daemons up, doing startup-complete notify
# Aug 16 19:03:03 router frrinit.sh[4610]:  * Started watchfrr
# Aug 16 19:03:03 router systemd[1]: Started frr.service - FRRouting.
```

### Cilium BGP 설정

#### 노드 라벨링 및 BGP Advertisement 구성

BGP를 사용할 노드를 라벨링하고 BGP 리소스를 생성합니다.

```bash
# BGP를 사용할 노드 라벨링
kubectl label nodes k8s-ctr k8s-w0 k8s-w1 enable-bgp=true
kubectl get node -l enable-bgp=true

# BGP 설정 적용
cat <<EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "PodCIDR"
---
apiVersion: cilium.io/v2
kind: CiliumBGPPeerConfig
metadata:
  name: cilium-peer
spec:
  timers:
    holdTimeSeconds: 9
    keepAliveTimeSeconds: 3
  ebgpMultihop: 2
  gracefulRestart:
    enabled: true
    restartTimeSeconds: 15
  families:
    - afi: ipv4
      safi: unicast
      advertisements:
        matchLabels:
          advertise: "bgp"
---
apiVersion: cilium.io/v2
kind: CiliumBGPClusterConfig
metadata:
  name: cilium-bgp
spec:
  nodeSelector:
    matchLabels:
      "enable-bgp": "true"
  bgpInstances:
  - name: "instance-65001"
    localASN: 65001
    peers:
    - name: "tor-switch"
      peerASN: 65000
      peerAddress: 192.168.10.200
      peerConfigRef:
        name: "cilium-peer"
EOF

# router 노드의 로그에서 bgp_update 시그널 확인 가능
# sudo journalctl -u frr -f
# Aug 16 19:09:26 router bgpd[4638]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.20.100 in vrf default
# Aug 16 19:09:26 router bgpd[4638]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.101 in vrf default
# Aug 16 19:09:26 router bgpd[4638]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.100 in vrf default

```

### BGP 연결 확인

```bash
# BGP 피어 상태 확인
cilium bgp peers

# Node      Local AS   Peer AS   Peer Address     Session State   Uptime   Family         Received   Advertised
# k8s-ctr   65001      65000     192.168.10.200   established     38s      ipv4/unicast   4          2    
# k8s-w0    65001      65000     192.168.10.200   established     39s      ipv4/unicast   4          2    
# k8s-w1    65001      65000     192.168.10.200   established     39s      ipv4/unicast   4          2 

# BGP 라우트 확인
cilium bgp routes available ipv4 unicast

# Node      VRouter   Prefix          NextHop   Age    Attrs
# k8s-ctr   65001     172.20.0.0/24   0.0.0.0   1m7s   [{Origin: i} {Nexthop: 0.0.0.0}]   
# k8s-w0    65001     172.20.2.0/24   0.0.0.0   1m7s   [{Origin: i} {Nexthop: 0.0.0.0}]   
# k8s-w1    65001     172.20.1.0/24   0.0.0.0   1m7s   [{Origin: i} {Nexthop: 0.0.0.0}]

# 이 과정을 통해 Cilium이 BGP를 통해 외부 라우터와 정상적으로 통신하고, Kubernetes Pod 네트워크 대역이 외부 네트워크로 광고되고 있는지 확인할 수 있습니다.

# CiliumBGP 리소스 확인
kubectl get ciliumbgpadvertisements,ciliumbgppeerconfigs,ciliumbgpclusterconfigs

# Router에서 BGP 라우팅 테이블 확인
vagrant ssh router
sudo vtysh -c 'show ip bgp summary'
sudo vtysh -c 'show ip bgp'
ip route | grep bgp

# 172.20.0.0/24 nhid 32 via 192.168.10.100 dev eth1 proto bgp metric 20 
# 172.20.1.0/24 nhid 28 via 192.168.10.101 dev eth1 proto bgp metric 20 
# 172.20.2.0/24 nhid 31 via 192.168.20.100 dev eth2 proto bgp metric 20
```

### 통신 문제 해결

#### Pod 간 통신을 위한 라우팅 설정

BGP로 PodCIDR 정보는 교환되지만, Cilium은 기본적으로 외부 경로를 커널 라우팅 테이블에 주입하지 않습니다. <br/>
따라서 이 경우 수동으로 라우팅 테이블을 추가해야 합니다.

```bash
# k8s 파드 대역(172.20.0.0/16)에 대한 라우팅 추가
sudo ip route add 172.20.0.0/16 via 192.168.10.200
sshpass -p vagrant ssh vagrant@k8s-w1 "sudo ip route add 172.20.0.0/16 via 192.168.10.200"
sshpass -p vagrant ssh vagrant@k8s-w0 "sudo ip route add 172.20.0.0/16 via 192.168.20.200"

# 각 노드에서 라우팅 테이블(ip route) 확인
ip route | grep 172.20
sshpass -p vagrant ssh vagrant@k8s-w1 "ip route | grep 172.20"
sshpass -p vagrant ssh vagrant@k8s-w0 "ip route | grep 172.20"

# 통신 테스트 (성공)
kubectl exec -it curl-pod -- curl -s webpod | grep Hostname

# Hubble로 플로우 모니터링
cilium hubble port-forward&
hubble observe -f --protocol tcp --pod curl-pod

# Aug 16 15:00:53.736: default/curl-pod:38124 (ID:7810) <- default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK, PSH)
# Aug 16 15:00:53.737: default/curl-pod:38124 (ID:7810) -> default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK, FIN)
# Aug 16 15:00:53.737: default/curl-pod:38124 (ID:7810) <- default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK, FIN)
# Aug 16 15:00:53.737: default/curl-pod:38124 (ID:7810) -> default/webpod-65794679cc-7dbj2:80 (ID:3216) to-endpoint FORWARDED (TCP Flags: ACK)
```

---

## Service LoadBalancer IP BGP 광고

### LoadBalancer IP Pool 생성

BGP를 통해 Service LoadBalancer IP를 외부에 광고하기 위해 먼저 IP Pool을 생성합니다.

```bash
# IP Pool 정의
cat << EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "cilium-pool"
spec:
  allowFirstLastIPs: "No"
  blocks:
  - cidr: "172.16.1.0/24"
EOF

# Pool 상태 확인
kubectl get ippool

# Service를 LoadBalancer로 변경
kubectl patch svc webpod -p '{"spec": {"type": "LoadBalancer"}}'

# External IP 확인
kubectl get svc webpod
LBIP=$(kubectl get svc webpod -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "LoadBalancer IP: $LBIP"
```

### LoadBalancer IP BGP 광고 설정

LoadBalancer IP를 BGP 프로토콜로 광고하도록 설정합니다.

```bash
cat << EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements-lb-exip-webpod
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "Service"
      service:
        addresses:
          - LoadBalancerIP
      selector:             
        matchExpressions:
          - { key: app, operator: In, values: [ webpod ] }
EOF

# BGP 라우트 정책 확인
kubectl exec -it -n kube-system ds/cilium -- cilium-dbg bgp route-policies

# VRouter   Policy Name                                             Type     Match Peers         Match Families   Match Prefixes (Min..Max Len)   RIB Action   Path Actions
# 65001     allow-local                                             import                                                                        accept       
# 65001     tor-switch-ipv4-PodCIDR                                 export   192.168.10.200/32                    172.20.1.0/24 (24..24)          accept       
# 65001     tor-switch-ipv4-Service-webpod-default-LoadBalancerIP   export   192.168.10.200/32                    172.16.1.1/32 (32..32)          accept 

# Router에서 BGP 라우트 확인
sshpass -p 'vagrant' ssh vagrant@router ip -c route | grep bgp

# VIP 생성 확인
# 172.16.1.1 nhid 40 proto bgp metric 20 
# 172.20.0.0/24 nhid 32 via 192.168.10.100 dev eth1 proto bgp metric 20 
# 172.20.1.0/24 nhid 28 via 192.168.10.101 dev eth1 proto bgp metric 20 
# 172.20.2.0/24 nhid 31 via 192.168.20.100 dev eth2 proto bgp metric 20 

sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp 172.16.1.1/32'"

# VIP 에 대한 BGP Routing Table 생성 확인
# BGP routing table entry for 172.16.1.1/32, version 11
# Paths: (3 available, best #2, table default)
#   Advertised to non peer-group peers:
#   192.168.10.100 192.168.10.101 192.168.20.100
#   65001
#     192.168.20.100 from 192.168.20.100 (192.168.20.100)
#       Origin IGP, valid, external, multipath
#       Last update: Sat Aug 16 19:29:12 2025
#   65001
#     192.168.10.100 from 192.168.10.100 (192.168.10.100)
#       Origin IGP, valid, external, multipath, best (Router ID)
#       Last update: Sat Aug 16 19:29:12 2025
#   65001
#     192.168.10.101 from 192.168.10.101 (192.168.10.101)
#       Origin IGP, valid, external, multipath
#       Last update: Sat Aug 16 19:29:12 2025

# Router에서 LoadBalancer IP로 접근 (정상 확인)
sshpass -p 'vagrant' ssh vagrant@router curl -s $LBIP | grep Hostname
```

### External Traffic Policy

#### Local 모드 설정 (소스 IP 보존)

External Traffic Policy를 Local로 설정하면 실제 파드가 있는 노드만 BGP로 광고됩니다.

```bash
# External Traffic Policy를 Local로 변경
kubectl patch service webpod -p '{"spec":{"externalTrafficPolicy":"Local"}}'

# BGP 라우트 확인 (파드가 있는 노드만 광고)
sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp'"
sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp 172.16.1.1/32'"
sshpass -p 'vagrant' ssh vagrant@router "ip -c route"

# 접속 테스트
LBIP=172.16.1.1
for i in {1..100}; do
  curl -s $LBIP | grep Hostname
done | sort | uniq -c | sort -nr
```

#### ECMP Hash Policy 최적화

ECMP(Equal Cost Multi-Path)로 로드밸런싱 효율을 높이기 위해 Hash Policy를 최적화합니다.

```bash
# Linux ECMP Hash Policy를 L4 기반으로 변경
sshpass -p 'vagrant' ssh vagrant@router << 'EOF'
sudo sysctl -w net.ipv4.fib_multipath_hash_policy=1
sudo echo "net.ipv4.fib_multipath_hash_policy=1" >> /etc/sysctl.conf
EOF

# 부하분산 테스트
for i in {1..100}; do
  curl -s $LBIP | grep Hostname
done | sort | uniq -c | sort -nr
```

---

## ClusterMesh로 멀티 클러스터 구성

### Kind 클러스터 배포

ClusterMesh 실습을 위해 Kind(Kubernetes in Docker)로 두 개의 클러스터를 생성합니다.

#### Kind 설치

Kind(Kubernetes in Docker)가 설치되어 있지 않다면 다음 명령어로 설치합니다.

```bash
brew install kind
```

#### West/East 클러스터 생성

```bash
# West 클러스터 생성
kind create cluster --name west --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
- role: worker
networking:
  podSubnet: "10.0.0.0/16"
  serviceSubnet: "10.2.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# East 클러스터 생성
kind create cluster --name east --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 31000
    hostPort: 31000
  - containerPort: 31001
    hostPort: 31001
- role: worker
networking:
  podSubnet: "10.1.0.0/16"
  serviceSubnet: "10.3.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# 컨텍스트 확인
kubectl config get-contexts

# alias 설정
alias kwest='kubectl --context kind-west'
alias keast='kubectl --context kind-east'

# 노드 확인
kwest get node -owide
keast get node -owide

# NAME                 STATUS     ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION                         CONTAINER-RUNTIME
# west-control-plane   NotReady   control-plane   3m25s   v1.33.2   192.168.97.3   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
# west-worker          NotReady   <none>          3m12s   v1.33.2   192.168.97.2   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
# NAME                 STATUS     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION                         CONTAINER-RUNTIME
# east-control-plane   NotReady   control-plane   62s   v1.33.2   192.168.97.4   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
# east-worker          NotReady   <none>          48s   v1.33.2   192.168.97.5   <none>        Debian GNU/Linux 12 (bookworm)   6.14.10-orbstack-00291-g1b252bd3edea   containerd://2.1.3
```

### Cilium CNI 설치

ClusterMesh를 위한 Cilium을 각 클러스터에 설치합니다.

```bash
# Cilium CNI 설치
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "arm64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-darwin-${CLI_ARCH}.tar.gz{,.sha256sum}
shasum -a 256 -c cilium-darwin-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-darwin-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-darwin-${CLI_ARCH}.tar.gz{,.sha256sum}


> **⚠️ 주의:** 아래 내용은 1.18.0 테스트시 글로벌 서비스 클러스터 메시 분산처리가 정상적으로 이루어지지 않았습니다. 테스트를 해보신 분들은 진행 하신 후 기존 컨텍스트의 Cilium 을 날리고 1.17.6 버전으로 다시 테스트 하시길 추천드립니다.

# Cilium 기존 배포 삭제
cilium uninstall --context kind-west
cilium uninstall --context kind-east

# West 클러스터 Cilium 설치
cilium install --version 1.17.6 \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set bpf.masquerade=true \
  --set endpointHealthChecking.enabled=false \
  --set healthChecking=false \
  --set operator.replicas=1 \
  --set debug.enabled=true \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  --set ipv4NativeRoutingCIDR=10.0.0.0/16 \
  --set ipMasqAgent.enabled=true \
  --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.1.0.0/16}' \
  --set cluster.name=west \
  --set cluster.id=1 \
  --context kind-west

# East 클러스터 Cilium 설치
cilium install --version 1.17.6 \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set bpf.masquerade=true \
  --set endpointHealthChecking.enabled=false \
  --set healthChecking=false \
  --set operator.replicas=1 \
  --set debug.enabled=true \
  --set routingMode=native \
  --set autoDirectNodeRoutes=true \
  --set ipv4NativeRoutingCIDR=10.1.0.0/16 \
  --set ipMasqAgent.enabled=true \
  --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.0.0.0/16}' \
  --set cluster.name=east \
  --set cluster.id=2 \
  --context kind-east

# 상태 확인
cilium status --context kind-west
cilium status --context kind-east
```

### ClusterMesh 설정

#### 클러스터 연결

두 클러스터를 ClusterMesh로 연결합니다.

```bash
# Shared Certificate Authority 설정
keast delete secret -n kube-system cilium-ca
kubectl --context kind-west get secret -n kube-system cilium-ca -o yaml | \
kubectl --context kind-east create -f -

# ClusterMesh 활성화
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-west
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-east

# 클러스터 연결
cilium clustermesh connect --context kind-west --destination-context kind-east

# 상태 확인
cilium clustermesh status --context kind-west --wait
cilium clustermesh status --context kind-east --wait

# 노드 목록 확인 (양쪽 클러스터 노드가 모두 보임)
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium node list
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium node list

# Name                      IPv4 Address   Endpoint CIDR   IPv6 Address   Endpoint CIDR   Source
# east/east-control-plane   192.168.97.4   10.1.0.0/24                                    clustermesh
# east/east-worker          192.168.97.5   10.1.1.0/24                                    clustermesh
# west/west-control-plane   192.168.97.3   10.0.0.0/24                                    local
# west/west-worker          192.168.97.2   10.0.1.0/24                                    custom-resource
# Name                      IPv4 Address   Endpoint CIDR   IPv6 Address   Endpoint CIDR   Source
# east/east-control-plane   192.168.97.4   10.1.0.0/24                                    custom-resource
# east/east-worker          192.168.97.5   10.1.1.0/24                                    local
# west/west-control-plane   192.168.97.3   10.0.0.0/24                                    clustermesh
# west/west-worker          192.168.97.2   10.0.1.0/24                                    clustermesh
```

### Global Service 배포

멀티 클러스터 환경에서 Global Service를 배포하여 클러스터 간 서비스 디스커버리를 구현합니다.

```bash
# 각 컨트롤 플레인 노드에 대하여 Pod 배치될 수 있도록 Taint 설정 해제
keast taint nodes east-control-plane node-role.kubernetes.io/control-plane:NoSchedule-
kwest taint nodes west-control-plane node-role.kubernetes.io/control-plane:NoSchedule-

# West 클러스터에 서비스 배포
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  annotations:
    service.cilium.io/global: "true"
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

# East 클러스터에도 동일 서비스 배포
cat << EOF | kubectl apply --context kind-east -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webpod
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  annotations:
    service.cilium.io/global: "true"
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF

kwest get svc -l app=webpod
kwest get pods -l app=webpod -owide
keast get svc -l app=webpod
keast get pods -l app=webpod -owide

# NAME     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
# webpod   ClusterIP   10.2.175.143   <none>        80/TCP    56s
# NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
# webpod-74f6f7bd86-5d54n   1/1     Running   0          56s   10.0.1.75    west-worker          <none>           <none>
# webpod-74f6f7bd86-tfrmk   1/1     Running   0          56s   10.0.0.103   west-control-plane   <none>           <none>

# NAME     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
# webpod   ClusterIP   10.3.128.57   <none>        80/TCP    50s
# NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE                 NOMINATED NODE   READINESS GATES
# webpod-74f6f7bd86-f5sq9   1/1     Running   0          50s   10.1.0.141   east-control-plane   <none>           <none>
# webpod-74f6f7bd86-zzg4b   1/1     Running   0          50s   10.1.1.36    east-worker          <none>           <none>

# 서비스 엔드포인트 확인
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 14   10.2.175.143:80/TCP      ClusterIP      1 => 10.0.1.75:80/TCP (active)        
#                                              2 => 10.0.0.103:80/TCP (active)       
#                                              3 => 10.1.0.141:80/TCP (active)       
#                                              4 => 10.1.1.36:80/TCP (active) 

keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 14   10.3.128.57:80/TCP       ClusterIP      1 => 10.0.0.103:80/TCP (active)       
#                                              2 => 10.0.1.75:80/TCP (active)        
#                                              3 => 10.1.0.141:80/TCP (active)       
#                                              4 => 10.1.1.36:80/TCP (active)

```

#### 테스트용 Pod 배포

각 클러스터에 테스트용 Pod를 배포하여 크로스 클러스터 통신을 확인합니다.

```bash
# West 클러스터에 curl-pod 배포
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# East 클러스터에 curl-pod 배포
cat << EOF | kubectl apply --context kind-east -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# 통신 테스트 (양쪽 클러스터의 Pod에서 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

kubectl exec -it curl-pod --context kind-east -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# Local affinity 설정
kwest annotate service webpod service.cilium.io/affinity=local --overwrite
keast annotate service webpod service.cilium.io/affinity=local --overwrite

# 상태 확인
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 자기 위치의 노드에 있는 EndPoint preferred 마킹 추가
# 14   10.2.175.143:80/TCP      ClusterIP      1 => 10.0.1.75:80/TCP (active) (preferred)    
#                                              2 => 10.0.0.103:80/TCP (active) (preferred)   
#                                              3 => 10.1.0.141:80/TCP (active)               
#                                              4 => 10.1.1.36:80/TCP (active) 

keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity

# 자기 위치의 노드에 있는 EndPoint preferred 마킹 추가
# 14   10.3.128.57:80/TCP       ClusterIP      1 => 10.0.0.103:80/TCP (active)               
#                                              2 => 10.0.1.75:80/TCP (active)                
#                                              3 => 10.1.0.141:80/TCP (active) (preferred)   
#                                              4 => 10.1.1.36:80/TCP (active) (preferred)

# 테스트 (로컬 클러스터의 Pod에서만 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# 자기 위치의 노드에 EndPoint 가 없는 상태 테스트
kwest scale deployment webpod --replicas 0

# 테스트 (다른 클러스터의 Pod에서만 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# 원복
kwest scale deployment webpod --replicas 2

# Remote affinity 설정
kwest annotate service webpod service.cilium.io/affinity=remote --overwrite
keast annotate service webpod service.cilium.io/affinity=remote --overwrite

# 테스트 (원격 클러스터의 Pod에서 응답)
kubectl exec -it curl-pod --context kind-west -- sh -c \
  'while true; do curl -s --connect-timeout 1 webpod; sleep 1; echo "---"; done;'

# 동작 확인 모두완료, 전체 Kind 클러스터 정리
kind delete clusters east
kind delete clusters west
```

---

## Multi-Cluster Services (MCS) API

### MCS API 소개

Kubernetes Multi-Cluster Services API는 KEP-1645에서 정의된 표준 API로, 여러 클러스터에 걸쳐 서비스를 노출하고 사용하는 표준화된 방법을 제공합니다. Cilium은 v1.17부터 MCS API를 지원하여 ClusterMesh 기반의 멀티 클러스터 서비스 디스커버리를 제공합니다.

**MCS API의 주요 리소스:**
- **ServiceExport**: 서비스를 다른 클러스터에 노출하기 위한 리소스
- **ServiceImport**: 다른 클러스터에서 노출된 서비스를 가져오기 위한 리소스 (자동 생성)

### MCS API 설정

#### Kind 클러스터 실습환경 구성

```bash
# West 클러스터
kind create cluster --name west --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
networking:
  podSubnet: "10.0.0.0/16"
  serviceSubnet: "10.2.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# East 클러스터
kind create cluster --name east --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
networking:
  podSubnet: "10.1.0.0/16"
  serviceSubnet: "10.3.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# alias 설정
alias kwest='kubectl --context kind-west'
alias keast='kubectl --context kind-east'
```

#### MCS API CRD 설치

```bash
# MCS API CRDs 설치 (West 클러스터)
kubectl apply --context kind-west -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/master/config/crd/multicluster.x-k8s.io_serviceexports.yaml
kubectl apply --context kind-west -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/master/config/crd/multicluster.x-k8s.io_serviceimports.yaml

# MCS API CRDs 설치 (East 클러스터)
kubectl apply --context kind-east -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/master/config/crd/multicluster.x-k8s.io_serviceexports.yaml
kubectl apply --context kind-east -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/master/config/crd/multicluster.x-k8s.io_serviceimports.yaml

# CRD 확인
kwest get crd | grep multicluster
keast get crd | grep multicluster
```

#### Cilium 설치 (MCS API 지원 포함)

```bash
# West 클러스터 Cilium 설치
cilium install --version 1.18.1 \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set cluster.name=west \
  --set cluster.id=1 \
  --context kind-west

# East 클러스터 Cilium 설치
cilium install --version 1.18.1 \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set cluster.name=east \
  --set cluster.id=2 \
  --context kind-east

# Helm repo 추가
helm repo add cilium https://helm.cilium.io/
helm repo update

# MCS API 지원 활성화
kubectl config use-context kind-west
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system \
  --reuse-values \
  --set clustermesh.enableMCSAPISupport=true


kubectl config use-context kind-east
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system \
  --reuse-values \
  --set clustermesh.enableMCSAPISupport=true

# Cilium 재시작
kubectl rollout restart ds/cilium -n kube-system --context kind-west
kubectl rollout restart ds/cilium -n kube-system --context kind-east

# 활성화 여부 확인
cilium config view --context kind-west | grep mcs
cilium config view --context kind-east | grep mcs
```

#### CoreDNS 멀티클러스터 플러그인 설정

MCS API를 완전히 활용하려면 CoreDNS에 multicluster 플러그인을 추가해야 합니다. 이 플러그인은 `clusterset.local` 도메인을 통해 서비스를 조회할 수 있게 해줍니다.

> **⚠️ 주의:** 기본 CoreDNS는 multicluster 플러그인을 포함하지 않으므로 재컴파일이 필요합니다.

##### CoreDNS 재컴파일 (옵션)

```bash
# CoreDNS 소스 클론
git clone https://github.com/coredns/coredns.git
cd coredns
git checkout v1.11.4

# plugins.cfg 파일에 multicluster 플러그인 추가
# kubernetes 플러그인 바로 아래에 추가
...
kubernetes:kubernetes
multicluster:github.com/coredns/multicluster
...

# CoreDNS 빌드 (arm64)
GOOS=linux GOARCH=arm64 make

# Docker 이미지 빌드
# 리눅스 aarch64(arm64) 아키텍처로 빌드하려면 아래와 같이 --platform 옵션을 변경하세요.
docker build --platform=linux/arm64 -t coredns:multicluster .

# kind 클러스터의 컨트롤 플레인 노드에 이미지 직접 로드
kind load docker-image coredns:multicluster --name west
kind load docker-image coredns:multicluster --name east
```

##### CoreDNS RBAC 권한 추가

ServiceImport 리소스를 읽을 수 있도록 CoreDNS에 권한을 부여합니다.

```bash
# West 클러스터
kubectl patch clusterrole system:coredns --type=json --context kind-west \
   -p='[{"op":"add","path":"/rules/-","value":{"apiGroups":["multicluster.x-k8s.io"],"resources":["serviceimports"],"verbs":["list","watch"]}}]'

# East 클러스터
kubectl patch clusterrole system:coredns --type=json --context kind-east \
   -p='[{"op":"add","path":"/rules/-","value":{"apiGroups":["multicluster.x-k8s.io"],"resources":["serviceimports"],"verbs":["list","watch"]}}]'
```

##### CoreDNS Corefile 수정

multicluster 플러그인을 활성화하기 위해 Corefile을 수정합니다.

```bash
# West 클러스터
kubectl edit configmap -n kube-system coredns --context kind-west
kubectl get configmap -n kube-system coredns --context kind-west -oyaml

# East 클러스터  
kubectl edit configmap -n kube-system coredns --context kind-east
kubectl get configmap -n kube-system coredns --context kind-east -oyaml

# Corefile: |
#   .:53 {
#       errors
#       health {
#          lameduck 5s
#       }
#       ready
#       multicluster clusterset.local
#       kubernetes cluster.local in-addr.arpa ip6.arpa {
#          pods insecure
#          fallthrough in-addr.arpa ip6.arpa
#          ttl 30
#       }
#       prometheus :9153
#       forward . /etc/resolv.conf {
#          max_concurrent 1000
#       }
#       cache 30 {
#          disable success cluster.local
#          disable denial cluster.local
#       }
#       loop
#       reload
#       loadbalance
#   }

# CoreDNS 재시작 및 재배포
kubectl set image deployment/coredns -n kube-system coredns=coredns:multicluster --context kind-west
kubectl set image deployment/coredns -n kube-system coredns=coredns:multicluster --context kind-east

kubectl rollout restart deployment/coredns -n kube-system --context kind-west
kubectl rollout restart deployment/coredns -n kube-system --context kind-east
```

> **💡 참고:** Cilium의 MCS API 구현은 CoreDNS multicluster 플러그인 없이도 기본적인 동작은 가능하지만, `clusterset.local` 도메인을 통한 표준 DNS 조회는 지원되지 않습니다. 프로덕션 환경에서는 multicluster 플러그인 설정을 권장합니다.

### 서비스 내보내기 (Exporting a Service)

서비스를 내보내려면 ServiceExport 리소스를 생성해야 합니다. 그러면 해당 클러스터에 서비스 네임스페이스가 있는 경우 모든 클러스터로 서비스가 내보내집니다.

```yaml
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceExport
metadata:
  name: rebel-base
  namespace: default
```

모든 클러스터와 동일한 이름과 네임스페이스를 가진 각 내보낸 서비스 세트에 대해 ServiceImport 리소스가 자동으로 생성됩니다. 동일한 이름과 네임스페이스를 가진 해당 내보낸 서비스의 모든 엔드포인트는 병합되어 전역적으로 사용 가능하게 됩니다.

#### DNS 도메인 구조

MCS-API를 통해 내보낸 서비스는 기본적으로 `<svc>.<ns>.svc.clusterset.local` 도메인에서 사용할 수 있습니다. Pod에 호스트 이름(예: StatefulSet)을 정의한 경우, 각 Pod는 `<hostname>.<clustername>.<svc>.<ns>.svc.clusterset.local` 도메인을 통해서도 사용할 수 있습니다.

> **⚠️ 주의:** 특정 클러스터의 모든 서비스 엔드포인트를 가져올 수 있는 `<clustername>.<svc>.<ns>.svc.clusterset.local` 도메인은 허용되지 않습니다!

이러한 동작을 원하신다면 클러스터 및/또는 리전당 하나의 서비스를 생성하고 그에 따라 내보내는 것이 좋습니다. 예를 들어, 하나의 서비스만 생성하는 대신 `mysvc-eu`와 `mysvc-us` 서비스를 생성하고 내보내는 것이 좋습니다.

### ClusterMesh 연결 설정

MCS API가 작동하려면 먼저 ClusterMesh를 활성화하고 클러스터 간 연결을 설정해야 합니다.

```bash
# ClusterMesh 활성화
cilium clustermesh enable --context kind-west --service-type NodePort
cilium clustermesh enable --context kind-east --service-type NodePort

# 상태 확인 (Ready 상태까지 대기)
cilium clustermesh status --context kind-west --wait
cilium clustermesh status --context kind-east --wait

# 클러스터 연결
cilium clustermesh connect --context kind-west --destination-context kind-east

# 연결 상태 확인
cilium clustermesh status --context kind-west --wait
cilium clustermesh status --context kind-east --wait

# ⚠️  Service type NodePort detected! Service may fail when nodes are removed from the cluster!
# ✅ Service "clustermesh-apiserver" of type "NodePort" found
# ✅ Cluster access information is available:
#   - 192.168.97.3:32379
# ✅ Deployment clustermesh-apiserver is ready
# ℹ️  KVStoreMesh is enabled

# ✅ All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]
# ✅ All 1 KVStoreMesh replicas are connected to all clusters [min:1 / avg:1.0 / max:1]

# 🔌 Cluster Connections:
#   - east: 2/2 configured, 2/2 connected - KVStoreMesh: 1/1 configured, 1/1 connected

# 🔀 Global services: [ min:3 / avg:3.0 / max:3 ]

# ⚠️  Service type NodePort detected! Service may fail when nodes are removed from the cluster!
# ✅ Service "clustermesh-apiserver" of type "NodePort" found
# ✅ Cluster access information is available:
#   - 192.168.97.5:32379
# ✅ Deployment clustermesh-apiserver is ready
# ℹ️  KVStoreMesh is enabled

# ✅ All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]
# ✅ All 1 KVStoreMesh replicas are connected to all clusters [min:1 / avg:1.0 / max:1]

# 🔌 Cluster Connections:
#   - west: 2/2 configured, 2/2 connected - KVStoreMesh: 1/1 configured, 1/1 connected

# 🔀 Global services: [ min:3 / avg:3.0 / max:3 ]
```

### MCS-API를 사용한 간단한 예제 서비스 배포

#### Cluster 1 (West) 배포

```bash
# Cilium 예제 서비스 배포
kubectl apply --context kind-west -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/clustermesh/cluster1.yaml

# MCS API 예제 배포
kubectl apply --context kind-west -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/clustermesh/mcsapi-example.yaml
```

#### Cluster 2 (East) 배포

```bash
# Cilium 예제 서비스 배포
kubectl apply --context kind-east -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/clustermesh/cluster2.yaml

# MCS API 예제 배포
kubectl apply --context kind-east -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/clustermesh/mcsapi-example.yaml
```

#### 내보낸 서비스 액세스

두 클러스터 중 하나에서 내보낸 서비스에 액세스합니다.

```bash
# West 클러스터에서 테스트
kubectl exec -ti deployment/x-wing --context kind-west -- curl rebel-base-mcsapi.default.svc.clusterset.local

# 또는 여러 번 요청하여 클러스터 분산 확인
for i in {1..10}; do
  kubectl exec deployment/x-wing --context kind-west -- curl -s rebel-base-mcsapi.default.svc.clusterset.local
done

# 두 클러스터의 Pod에서 응답을 확인할 수 있습니다
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-2"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-2"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-2"}
# {"Galaxy": "Alderaan", "Cluster": "Cluster-1"}
```

> **💡 참고:** clusterset.local 도메인이 작동하려면 CoreDNS multicluster 플러그인이 필요합니다. 플러그인이 없는 경우, ServiceImport의 ClusterIP를 직접 사용할 수 있습니다:
> ```bash
> # ServiceImport IP 확인
> SVC_IP=$(kubectl get serviceimport rebel-base-mcsapi -o jsonpath='{.spec.ips[0]}' --context kind-west)
> kubectl exec deployment/x-wing --context kind-west -- curl $SVC_IP
> ```

### ServiceImport 상태 확인

```bash
# ServiceImport 자동 생성 확인
kwest get serviceimport rebel-base-mcsapi -n default
keast get serviceimport rebel-base-mcsapi -n default

# NAME                TYPE           IP                 AGE
# rebel-base-mcsapi   ClusterSetIP   ["10.2.235.195"]   32s
# NAME                TYPE           IP                 AGE
# rebel-base-mcsapi   ClusterSetIP   ["10.3.191.219"]   32s

```

### ClusterMesh vs MCS API 비교

#### ClusterMesh 방식

**장점:**
- ✅ Cilium 네이티브 기능으로 설정이 간단
- ✅ `service.cilium.io/global` 어노테이션만으로 글로벌 서비스 활성화
- ✅ Service Affinity(local/remote) 세밀한 제어 가능
- ✅ 기존 DNS 이름 그대로 사용 가능
- ✅ 실시간 엔드포인트 상태 추적
- ✅ Hubble을 통한 가시성

**단점:**
- ❌ Cilium 전용 기능으로 벤더 종속성
- ❌ 표준 Kubernetes API가 아님
- ❌ 다른 CNI/Service Mesh와 호환성 제한

#### MCS API 방식

**장점:**
- ✅ Kubernetes 표준 API (KEP-1645)
- ✅ 벤더 중립적인 접근 방식
- ✅ 다른 Service Mesh 솔루션과 호환 가능
- ✅ 명시적인 서비스 노출 제어 (ServiceExport)
- ✅ Gateway API와 통합 가능

**단점:**
- ❌ 추가 CRD 설치 필요
- ❌ CoreDNS 설정 변경 필요 (clusterset.local 도메인)
- ❌ 아직 베타 기능으로 성숙도 낮음
- ❌ Service Affinity 같은 고급 기능 제한
- ❌ Cilium에서는 실질적으로 ClusterMesh 위에서 동작

```bash
# 전체 Kind 클러스터 정리
kind delete clusters east
kind delete clusters west
```

---
