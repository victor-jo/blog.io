---
layout: post
title: "Cilium 네트워킹 핸즈온 가이드 (4): Service Mesh"
date: 2025-08-23 10:00:00 +0900
categories: cilium kubernetes
tags: [cilium, service-mesh, ingress, gateway-api, mutual-authentication, envoy, l7-proxy, kubernetes]
---

## Cilium 네트워킹 핸즈온 가이드 (4)

Cilium은 단순한 CNI 외에 Service Mesh 기능을 제공합니다.  
이번 포스트에서는 Cilium의 Service Mesh 기능인 Ingress, Gateway API, L7 Traffic Management를 실습을 통해 알아보겠습니다.

### 실습 환경 소개

Cilium Service Mesh 기능을 테스트하기 위한 환경을 구성합니다.

- Control Plane(k8s-ctr): 192.168.10.100
- Worker Node 1(k8s-w1): 192.168.10.101  
- Router: 192.168.10.200 (라우팅 및 외부 접속 테스트용)

### 기본 환경 확인
```bash
# k8s-ctr 노드 접속
vagrant ssh k8s-ctr

# 클러스터 정보 확인
kubectl cluster-info
kubectl get node -owide

# Cilium 설치 상태 확인
cilium status
cilium config view | grep -E '^loadbalancer|l7'

# enable-l7-proxy                                   true
# loadbalancer-l7                                   envoy
# loadbalancer-l7-algorithm                         round_robin
```

---

## Cilium Service Mesh 소개

### Service Mesh란?

Service Mesh는 마이크로서비스 간 통신을 관리하는 인프라 계층입니다. 애플리케이션 코드 수정 없이 트래픽 관리, 보안, 관찰성을 제공합니다.

### Cilium Service Mesh의 특징

Cilium Service Mesh는 eBPF와 Envoy를 결합한 아키텍처를 가집니다.

- **L3/L4 수준**: eBPF를 통한 고성능 커널 내부 데이터 경로
- **L7 수준**: Envoy 프록시를 통한 애플리케이션 계층 처리
- **사이드카 없는 아키텍처**: 노드별 Envoy 프록시 사용

### 샘플 애플리케이션 배포
```bash
# Bookinfo 데모 애플리케이션 배포 (ARM 호환 버전)
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.26/samples/bookinfo/platform/kube/bookinfo.yaml

# 배포 확인
kubectl get pod,svc,ep

# 테스트용 curl 파드 배포
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail", "-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF
```

---

## Kubernetes Ingress 지원

### Cilium Ingress Controller 활성화

Cilium은 표준 Kubernetes Ingress 리소스를 지원하며, 내장된 Envoy 프록시를 통해 L7 라우팅을 제공합니다.

```bash
# Cilium Ingress 활성화 확인
kubectl get ingressclasses.networking.k8s.io

# NAME     CONTROLLER                     PARAMETERS   AGE
# cilium   cilium.io/ingress-controller   <none>       49m

# Cilium Ingress 서비스 확인
kubectl get svc -n kube-system cilium-ingress

# NAME             TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE
# cilium-ingress   LoadBalancer   10.96.207.154   192.168.10.211   80:32642/TCP,443:31427/TCP   49m
```

### LoadBalancer IP Pool 설정
```bash
# LB-IPAM 설정으로 External IP 할당
cat << EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "cilium-lb-ippool"
spec:
  blocks:
  - start: "192.168.10.211"
    stop:  "192.168.10.215"
EOF

# L2 Announcement 정책 설정
cat << EOF | kubectl apply -f -
apiVersion: "cilium.io/v2alpha1"
kind: CiliumL2AnnouncementPolicy
metadata:
  name: policy1
spec:
  interfaces:
  - eth1
  externalIPs: true
  loadBalancerIPs: true
EOF

kubectl get ippool

# NAME               DISABLED   CONFLICTING   IPS AVAILABLE   AGE
# cilium-lb-ippool   false      False         4               5m23s
```

### HTTP Ingress 예제
```bash
# Basic Ingress 생성
cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-ingress
spec:
  ingressClassName: cilium
  rules:
  - http:
      paths:
      - backend:
          service:
            name: details
            port:
              number: 9080
        path: /details
        pathType: Prefix
      - backend:
          service:
            name: productpage
            port:
              number: 9080
        path: /
        pathType: Prefix
EOF

# Ingress 확인
kubectl get ingress
LBIP=$(kubectl get ingress basic-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo $LBIP

# 호출 테스트
curl -s http://$LBIP/
curl -s http://$LBIP/details/1

# {"id":1,"author":"William Shakespeare","year":1595,"type":"paperback","pages":200,"publisher":"PublisherA","language":"English","ISBN-10":"1234567890","ISBN-13":"123-1234567890"}
```

### TLS Termination 설정
```bash
# TLS 인증서 생성 (mkcert 사용)
apt install mkcert -y
mkcert '*.cilium.rocks'

# Kubernetes Secret 생성
kubectl create secret tls demo-cert \
  --key=_wildcard.cilium.rocks-key.pem \
  --cert=_wildcard.cilium.rocks.pem

# TLS Ingress 생성
cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
spec:
  ingressClassName: cilium
  rules:
  - host: bookinfo.cilium.rocks
    http:
      paths:
      - backend:
          service:
            name: details
            port:
              number: 9080
        path: /details
        pathType: Prefix
      - backend:
          service:
            name: productpage
            port:
              number: 9080
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - bookinfo.cilium.rocks
    secretName: demo-cert
EOF

# mkcert CA 설치 및 테스트
mkcert -install
LBIP=$(kubectl get ingress tls-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl -s --resolve bookinfo.cilium.rocks:443:${LBIP} https://bookinfo.cilium.rocks/
curl -s --resolve bookinfo.cilium.rocks:443:${LBIP} https://bookinfo.cilium.rocks/details/1

# {"id":1,"author":"William Shakespeare","year":1595,"type":"paperback","pages":200,"publisher":"PublisherA","language":"English","ISBN-10":"1234567890","ISBN-13":"123-1234567890"}
```

---

## Gateway API 지원

### Gateway API CRD 설치

Gateway API는 Ingress의 후속 표준으로 더 다양한 기능과 역할 기반 설정을 제공합니다.

```bash
# Gateway API CRDs 설치
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_gatewayclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_gateways.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_httproutes.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_referencegrants.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_grpcroutes.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/experimental/gateway.networking.k8s.io_tlsroutes.yaml

# 확인
kubectl get crd | grep gateway.networking.k8s.io

# gatewayclasses.gateway.networking.k8s.io     2025-08-23T02:41:56Z
# gateways.gateway.networking.k8s.io           2025-08-23T02:41:56Z
# grpcroutes.gateway.networking.k8s.io         2025-08-23T02:43:55Z
# httproutes.gateway.networking.k8s.io         2025-08-23T02:41:56Z
# referencegrants.gateway.networking.k8s.io    2025-08-23T02:41:57Z
# tlsroutes.gateway.networking.k8s.io          2025-08-23T02:43:56Z

# Cilium Gateway API 활성화
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system --reuse-values \
  --set ingressController.enabled=false \
  --set gatewayAPI.enabled=true

# Cilium 오퍼레이터 및 데몬셋 재시작
kubectl -n kube-system rollout restart deployment/cilium-operator
kubectl -n kube-system rollout restart ds/cilium

# GatewayClass 확인
kubectl get GatewayClass

# NAME     CONTROLLER                     ACCEPTED   AGE
# cilium   io.cilium/gateway-controller   True       5m27s
```

### Gateway 및 HTTPRoute 배포
```bash
# Gateway 생성
cat << EOF | kubectl apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: tls-gateway
spec:
  gatewayClassName: cilium
  listeners:
  - name: https-1
    protocol: HTTPS
    port: 443
    hostname: "bookinfo.cilium.rocks"
    tls:
      certificateRefs:
      - kind: Secret
        name: demo-cert
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: https-app-route-1
spec:
  parentRefs:
  - name: tls-gateway
  hostnames:
  - "bookinfo.cilium.rocks"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /details
    backendRefs:
    - name: details
      port: 9080
EOF

# Gateway 상태 확인
kubectl get gateway tls-gateway

# NAME          CLASS    ADDRESS          PROGRAMMED   AGE
# tls-gateway   cilium   192.168.10.214   True         67s

kubectl get httproutes https-app-route-1

# NAME                HOSTNAMES                   AGE
# https-app-route-1   ["bookinfo.cilium.rocks"]   74s

# HTTP 라우팅 테스트
GATEWAY2=$(kubectl get gateway tls-gateway -o jsonpath='{.status.addresses[0].value}')
echo $GATEWAY2

curl -s --resolve bookinfo.cilium.rocks:443:${GATEWAY2} https://bookinfo.cilium.rocks/details/1 | jq

# {
#   "id": 1,
#   "author": "William Shakespeare",
#   "year": 1595,
#   "type": "paperback",
#   "pages": 200,
#   "publisher": "PublisherA",
#   "language": "English",
#   "ISBN-10": "1234567890",
#   "ISBN-13": "123-1234567890"
# }
```

### TLS Passthrough
```bash
# HTTPS 지원 NGINX 배포
cat <<'EOF' > nginx.conf
events { }
http {
  server {
    listen 443 ssl;
    server_name nginx.cilium.rocks;
    ssl_certificate /etc/nginx-server-certs/tls.crt;
    ssl_certificate_key /etc/nginx-server-certs/tls.key;
    root /usr/share/nginx/html;
    index index.html;
  }
}
EOF

kubectl create configmap nginx-configmap --from-file=nginx.conf

# NGINX Deployment 및 Service
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  ports:
  - port: 443
    protocol: TCP
  selector:
    run: my-nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      run: my-nginx
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 443
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx
        - name: nginx-server-certs
          mountPath: /etc/nginx-server-certs
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-configmap
      - name: nginx-server-certs
        secret:
          secretName: demo-cert
EOF

# TLS Passthrough Gateway
cat << EOF | kubectl apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: cilium-tls-gateway
spec:
  gatewayClassName: cilium
  listeners:
  - name: https
    hostname: "nginx.cilium.rocks"
    port: 443
    protocol: TLS
    tls:
      mode: Passthrough
    allowedRoutes:
      namespaces:
        from: All
---
apiVersion: gateway.networking.k8s.io/v1alpha2
kind: TLSRoute
metadata:
  name: nginx
spec:
  parentRefs:
  - name: cilium-tls-gateway
  hostnames:
  - "nginx.cilium.rocks"
  rules:
  - backendRefs:
    - name: my-nginx
      port: 443
EOF

# TLS Passthrough 테스트
GATEWAY=$(kubectl get gateway cilium-tls-gateway -o jsonpath='{.status.addresses[0].value}')
curl -v --resolve "nginx.cilium.rocks:443:$GATEWAY" "https://nginx.cilium.rocks:443"
```

---

## L7-Aware Traffic Management

### Envoy 기반 L7 트래픽 관리

Cilium은 Envoy 프록시를 통해 L7 트래픽 관리 기능을 제공합니다.

```bash
# L7 Traffic Management 활성화
helm upgrade cilium cilium/cilium --version 1.18.1 \
  --namespace kube-system --reuse-values \
  --set envoyConfig.enabled=true \
  --set loadBalancer.l7.backend=envoy

kubectl -n kube-system rollout restart deployment/cilium-operator
kubectl -n kube-system rollout restart ds/cilium
```

### 샘플 애플리케이션 배포
```bash
# Echo 서비스 배포
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/test-application.yaml

# Client2 Pod 확인
export CLIENT2=$(kubectl get pods -l name=client2 -o jsonpath='{.items[0].metadata.name}')
echo $CLIENT2

# 기본 연결 테스트
kubectl exec -it $CLIENT2 -- curl -v echo-service-1:8080/
kubectl exec -it $CLIENT2 -- curl -v echo-service-2:8080/

# error: Internal error occurred: unable to upgrade connection: container not found ("client2")
# error: Internal error occurred: unable to upgrade connection: container not found ("client2")
```

### L7 네트워크 정책 적용
```bash
# L7 HTTP 정책 적용
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/client-egress-l7-http.yaml
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/client-egress-only-dns.yaml

# L7 정책 테스트
kubectl exec -it $CLIENT2 -- curl -v echo-service-1:8080/     # 성공
kubectl exec -it $CLIENT2 -- curl -v echo-service-1:8080/foo  # 403 Forbidden

# Hubble로 L7 트래픽 관찰
cilium hubble port-forward&
hubble observe --protocol http --from-pod default/$CLIENT2

# foo 에 대해 DROPPED
# Aug 23 12:52:57.916: default/client2-c97ddf6cf-mf5zs:51948 (ID:6914) -> default/echo-service-1-867d69c679-t9v4v:8080 (ID:5809) http-request FORWARDED (HTTP/1.1 GET http://echo-service-1:8080/)
# Aug 23 12:53:03.379: default/client2-c97ddf6cf-mf5zs:33878 (ID:6914) -> default/echo-service-1-867d69c679-t9v4v:8080 (ID:5809) http-request DROPPED (HTTP/1.1 GET http://echo-service-1:8080/foo)
```

### Envoy Load Balancing 및 URL Rewriting
```bash
# CiliumClusterwideEnvoyConfig 적용
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/envoy-traffic-management-test.yaml

# 설정 확인
kubectl get ccec

# 로드 밸런싱 테스트 (50:50 분산)
for i in {1..10}; do
  kubectl exec -it $CLIENT2 -- curl -s echo-service-1:8080/ | grep "echo-service"
done

# URL Rewriting 테스트 (/foo -> /)
kubectl exec -it $CLIENT2 -- curl -v echo-service-1:8080/foo  # 성공 -> Rewriting 시킴
kubectl exec -it $CLIENT2 -- curl -v echo-service-1:8080/bar  # 403 Forbidden
```

### L7 메트릭 수집
```bash
# Prometheus 메트릭 수집
curl -s http://localhost:9965/metrics | grep http

# TYPE hubble_metrics_http_handler_request_duration_seconds histogram
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.005"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.01"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.025"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.05"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.1"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.25"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="0.5"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="1"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="2.5"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="5"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="10"} 3
# hubble_metrics_http_handler_request_duration_seconds_bucket{code="200",le="+Inf"} 3
# hubble_metrics_http_handler_request_duration_seconds_sum{code="200"} 0.003174208
# hubble_metrics_http_handler_request_duration_seconds_count{code="200"} 3

# Envoy Admin UI 활성화 (선택사항)
helm upgrade cilium cilium/cilium -n kube-system --version 1.18.1 \
  --reuse-values \
  --set envoy.debug.admin.enabled=true \
  --set envoy.debug.admin.port=9901

# Envoy Admin 포트 포워딩
ENVOY_POD=$(kubectl -n kube-system get po -l k8s-app=cilium-envoy -o name | head -1)
kubectl -n kube-system port-forward $ENVOY_POD 9901:9901
# http://localhost:9901 접속

# 터널링이 필요할 경우,
# ssh -N -v -L 9901:localhost:9901 -p 2022 root@127.0.0.1
# vagrant 환경은 vagrant ssh-config 로 SSH config 값을 확인할 수 있다
```

---

## Circuit Breaker로 서비스 안정성 확보

### Circuit Breaker란?

Circuit Breaker는 장애가 발생한 서비스로의 반복적인 요청을 차단하여 시스템 전체의 안정성을 보호하는 패턴입니다. 네트워크 지연, 서비스 장애 등의 문제가 확산되는 것을 방지합니다.

### Circuit Breaker 테스트 환경 준비

```bash
# Fortio 테스트 애플리케이션 배포
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/test-application-proxy-circuit-breaker.yaml

# 배포 확인
kubectl get pods,svc | grep -E '(fortio|echo)'

# pod/echo-service-67788f4d97-ppjqm    2/2     Running   0          9m21s
# pod/fortio-deploy-74ffb9b4d6-w6tzv   1/1     Running   0          9m21s
# service/echo-service   ClusterIP   10.96.189.8    <none>        8080/TCP   9m21s
# service/fortio         ClusterIP   10.96.116.29   <none>        8080/TCP   9m22s

# Fortio Pod 이름 저장
export FORTIO_POD=$(kubectl get pods -l app=fortio -o jsonpath='{.items[0].metadata.name}')
echo $FORTIO_POD
```

### Circuit Breaker 설정

```bash
# Circuit Breaker 정책 적용
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/envoy-circuit-breaker.yaml

# apiVersion: cilium.io/v2
# kind: CiliumClusterwideEnvoyConfig
# metadata:
#   name: envoy-circuit-breaker
# spec:
#   services:
#     - name: echo-service
#       namespace: default
#   resources:
#     - "@type": type.googleapis.com/envoy.config.listener.v3.Listener
#       name: envoy-lb-listener
#       filter_chains:
#         - filters:
#             - name: envoy.filters.network.http_connection_manager
#               typed_config:
#                 "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
#                 stat_prefix: envoy-lb-listener
#                 rds:
#                   route_config_name: lb_route
#                 use_remote_address: true
#                 skip_xff_append: true
#                 http_filters:
#                   - name: envoy.filters.http.router
#                     typed_config:
#                       "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
#     - "@type": type.googleapis.com/envoy.config.route.v3.RouteConfiguration
#       name: lb_route
#       virtual_hosts:
#         - name: "lb_route"
#           domains: [ "*" ]
#           routes:
#             - match:
#                 prefix: "/"
#               route:
#                 weighted_clusters:
#                   clusters:
#                     - name: "default/echo-service"
#                       weight: 100
#     - "@type": type.googleapis.com/envoy.config.cluster.v3.Cluster
#       name: "default/echo-service"
#       connect_timeout: 5s
#       lb_policy: ROUND_ROBIN
#       type: EDS
#       edsClusterConfig:
#         serviceName: default/echo-service
#       circuit_breakers:
#         thresholds:
#         - priority: "DEFAULT"
#           max_requests: 2           # 최대 연결 수
#           max_pending_requests: 1   # 대기 큐의 최대 요청 수
#       outlier_detection:
#         split_external_local_origin_errors: true
#         consecutive_local_origin_failure: 2

# 설정 확인
kubectl get ccec envoy-circuit-breaker

# NAME                     AGE
# envoy-circuit-breaker    10s
```

### Circuit Breaker 동작 테스트

```bash
# 2개의 동시 연결로 20개 요청 전송
kubectl exec "$FORTIO_POD" -c fortio -- \
  /usr/bin/fortio load -c 2 -qps 0 -n 20 http://echo-service:8080

# Code 200 : 19 (95.0 %)
# Code 503 : 1 (5.0 %)
# Response Header Sizes : count 20 avg 370.5 +/- 85 min 0 max 390 sum 7410
# Response Body/Total Sizes : count 20 avg 2336.7 +/- 480.8 min 241 max 2447 sum 46734
# All done 20 calls (plus 0 warmup) 3.431 ms avg, 574.4 qps
```

대부분의 요청이 성공하지만, Circuit Breaker 임계값에 도달하면 일부 요청이 503 오류를 반환합니다.

### Circuit Breaking 테스트 (동시 연결 4개)

```bash
# 4개의 동시 연결로 20개 요청 전송
kubectl exec "$FORTIO_POD" -c fortio -- \
  /usr/bin/fortio load -c 4 -qps 0 -n 20 http://echo-service:8080

# Code 200 : 12 (60.0 %)
# Code 503 : 8 (40.0 %)
# Response Header Sizes : count 20 avg 234 +/- 191.1 min 0 max 390 sum 4680
# Response Body/Total Sizes : count 20 avg 1564.6 +/- 1081 min 241 max 2447 sum 31292
# All done 20 calls (plus 0 warmup) 9.892 ms avg, 364.9 qps
```

동시 연결이 설정된 임계값(max_requests: 2)을 초과하면 대부분의 요청이 Circuit Breaker에 의해 차단됩니다.

### 테스트 항목 정리

```bash
# ccec (CiliumClusterwideEnvoyConfig) 정리
kubectl delete ccec envoy-circuit-breaker

# 테스트 애플리케이션 제거
kubectl delete -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/test-application-proxy-circuit-breaker.yaml
```

---

## Traffic Shifting과 Canary 배포

### Traffic Shifting이란?

Traffic Shifting은 서비스의 여러 버전 간에 트래픽을 점진적으로 이동시키는 기능입니다. 이를 통해 새로운 버전을 안전하게 배포하고, 문제 발생 시 신속하게 롤백할 수 있습니다.

### 테스트 애플리케이션 배포

```bash
# Helloworld 애플리케이션 배포 (v1, v2)
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/client-helloworld.yaml

# 배포 확인
kubectl get pods --show-labels -o wide

# NAME                             READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES   LABELS
# client-85b7f79db-wxsv8           1/1     Running   0          4m18s   172.20.1.134   k8s-w1   <none>           <none>            kind=client,name=client,pod-template-hash=85b7f79db
# helloworld-v1-9c5dfd585-249rd    1/1     Running   0          4m18s   172.20.1.101   k8s-w1   <none>           <none>            app=helloworld,pod-template-hash=9c5dfd585,version=v1
# helloworld-v2-6f85d9d76f-lscsm   1/1     Running   0          4m18s   172.20.1.63    k8s-w1   <none>           <none>            app=helloworld,pod-template-hash=6f85d9d76f,version=v2

kubectl get svc --show-labels

# NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE     LABELS
# helloworld   ClusterIP   10.96.92.220   <none>        5000/TCP   4m27s   app=helloworld,service=helloworld
```

### 기본 라우팅 테스트

```bash
export CLIENT_POD=$(kubectl get pods -l name=client -o jsonpath='{.items[0].metadata.name}')

# 기본 라운드 로빈 동작 확인 (50:50 분산)
for i in {1..10}; do  kubectl exec -it $CLIENT_POD -- curl  helloworld:5000/hello; done

# Hello version: v2, instance: helloworld-v2-6f85d9d76f-2fm45
# Hello version: v1, instance: helloworld-v1-9c5dfd585-42hn7
# Hello version: v2, instance: helloworld-v2-6f85d9d76f-2fm45
# Hello version: v1, instance: helloworld-v1-9c5dfd585-42hn7
# Hello version: v1, instance: helloworld-v1-9c5dfd585-42hn7
# Hello version: v1, instance: helloworld-v1-9c5dfd585-42hn7
# Hello version: v2, instance: helloworld-v2-6f85d9d76f-2fm45
# Hello version: v1, instance: helloworld-v1-9c5dfd585-42hn7
# ...
```

### Traffic Shifting 설정 (90:10 비율)

```bash
# 백엔드 개별 서비스 생성
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/helloworld-service-v1-v2.yaml

kubectl get svc --show-labels

# NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   LABELS
# helloworld      ClusterIP   10.96.92.220    <none>        5000/TCP   22m   app=helloworld,service=helloworld
# helloworld-v1   ClusterIP   10.96.154.107   <none>        5000/TCP   8s    app=helloworld,service=helloworld,version=v1
# helloworld-v2   ClusterIP   10.96.233.0     <none>        5000/TCP   8s    app=helloworld,service=helloworld,version=v2

# 90% v1, 10% v2 트래픽 분산 설정
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.18.1/examples/kubernetes/servicemesh/envoy/envoy-helloworld-v1-90-v2-10.yaml

# 설정 확인
kubectl get cec

# NAME                AGE
# envoy-lb-listener   22s
```

### Traffic Shifting 테스트

```bash
# 100번 요청하여 트래픽 분산 비율 확인
for i in {1..100}; do
  kubectl exec $CLIENT_POD -- curl -s http://helloworld:5000/hello | grep -o "v[12]"
done | sort | uniq -c

# 거의 확실하게 90:10 으로 트래픽 분산됨
# 180 v1
#  20 v2
```


### 메트릭 모니터링

```bash
# Envoy 메트릭 확인
CILIUM_POD=$(kubectl -n kube-system get pods -l k8s-app=cilium -o jsonpath='{.items[0].metadata.name}')
kubectl -n kube-system port-forward $CILIUM_POD 9090:9090 &

# 클러스터별 트래픽 통계
curl -s localhost:9090/stats/prometheus | grep -E "cluster.*helloworld.*upstream_rq_total"

# envoy_cluster_default_helloworld_v1_upstream_rq_total{} 90
# envoy_cluster_default_helloworld_v2_upstream_rq_total{} 10

# 가중치 확인
curl -s localhost:9090/clusters | grep -A 5 "helloworld"
```

### 테스트 항목 정리

```bash
# CiliumEnvoyConfig 제거
kubectl delete cec --all

# 테스트 애플리케이션 제거
kubectl delete deployment helloworld-v1 helloworld-v2 client
kubectl delete service helloworld helloworld-v1 helloworld-v2
```

---
